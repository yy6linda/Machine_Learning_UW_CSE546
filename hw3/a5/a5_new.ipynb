{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "mndata = MNIST('./data/')\n",
    "x_train, y_train = map(np.array, mndata.load_training())\n",
    "x_test, y_test = map(np.array, mndata.load_testing())\n",
    "x_train = torch.FloatTensor(x_train/255.0)\n",
    "y_train = torch.tensor(y_train)\n",
    "x_test = torch.FloatTensor(x_test/255.0)\n",
    "y_test = torch.tensor(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W():\n",
    "    d = 784\n",
    "    h = 64\n",
    "    k = 10\n",
    "    alpha1 = (1/d)**0.5\n",
    "    alpha2 = (1/h)**0.5\n",
    "    W_0 = np.random.uniform(-alpha1, alpha1,(h,d))\n",
    "    W_1 = np.random.uniform(-alpha2, alpha2,(k,h))\n",
    "    b_0 = np.zeros((h,1))\n",
    "    b_1 = np.zeros((k,1))\n",
    "    W_0 = torch.FloatTensor(W_0).requires_grad_(True)\n",
    "    W_1 = torch.FloatTensor(W_1).requires_grad_(True)\n",
    "    b_0 = torch.FloatTensor(b_0).requires_grad_(True)\n",
    "    b_1 = torch.FloatTensor(b_1).requires_grad_(True)\n",
    "    return W_0,W_1,b_0,b_1\n",
    "\n",
    "def fit_gd_cross_entropy(x_train, y_train,x_test,y_test):\n",
    "    W_0,W_1,b_0,b_1 = init_W()\n",
    "    loss_prev = 0\n",
    "    d_loss = 1\n",
    "    loss = 1\n",
    "    accuracy = 0\n",
    "    ep = 0\n",
    "    batchsize = 1000\n",
    "    nbatch = int(x_train.shape[0]/batchsize)\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    traccuracy = 0\n",
    "    taccuracy = 0\n",
    "    while traccuracy <0.99:\n",
    "        ep = ep+1\n",
    "        accuracy_list = []\n",
    "        for i in range(0, nbatch):\n",
    "            x = x_train[i*batchsize:(i+1)*batchsize]\n",
    "            y = y_train[i*batchsize:(i+1)*batchsize]\n",
    "            optim = torch.optim.Adam([W_0,W_1,b_0,b_1], lr=1e-3)\n",
    "            x_1 = torch.matmul(W_0, x.T) + b_0\n",
    "            x_1 = torch.nn.functional.relu(x_1)\n",
    "            y_hat = torch.matmul(W_1, x_1) + b_1\n",
    "            #y_hat = torch.nn.functional.relu(y_hat)\n",
    "            y_hat = y_hat.T\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y.long())\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            accuracy = float(torch.sum(torch.argmax(y_hat, axis=1) == y)) / float(y.shape[0])\n",
    "            #accuracy_list.append(accuracy)\n",
    "            optim.step()\n",
    "        xtr_1 = torch.matmul(W_0, x_train.T) + b_0\n",
    "        xtr_1 = torch.nn.functional.relu(xtr_1)\n",
    "        ytr_hat = torch.matmul(W_1, xtr_1) + b_1\n",
    "        #yt_hat = torch.nn.functional.relu(yt_hat)\n",
    "        ytr_hat = ytr_hat.T\n",
    "        train_loss = torch.nn.functional.cross_entropy(ytr_hat, y_train.long())\n",
    "        traccuracy = float(torch.sum(torch.argmax(ytr_hat, axis=1) == y_train)) / float(y_train.shape[0])\n",
    "        xt_1 = torch.matmul(W_0, x_test.T) + b_0\n",
    "        xt_1 = torch.nn.functional.relu(xt_1)\n",
    "        yt_hat = torch.matmul(W_1, xt_1) + b_1\n",
    "        #yt_hat = torch.nn.functional.relu(yt_hat)\n",
    "        yt_hat = yt_hat.T\n",
    "        test_loss = torch.nn.functional.cross_entropy(yt_hat, y_test.long())\n",
    "        taccuracy = float(torch.sum(torch.argmax(yt_hat, axis=1) == y_test)) / float(y_test.shape[0])\n",
    "        train_accuracy_list.append(traccuracy)\n",
    "        test_accuracy_list.append(taccuracy)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        #print(\"epoch {}, accurracy on training dataset{}, accuracy on test dataset{} \".format(ep,traccuracy,taccuracy))\n",
    "    print(\"Finally epoch {}, accurracy on training dataset {}, accuracy on test dataset {}; loss on training {} loss on test{} \".format(ep,traccuracy,taccuracy,train_loss,test_loss))\n",
    "    return(train_accuracy_list, test_accuracy_list,ep,train_loss_list,test_loss_list)\n",
    "\n",
    "def plot(train_loss_list,ep,plot_name):\n",
    "    plt.clf()\n",
    "    sns.set()\n",
    "    plt.plot(range(1,ep+1),train_loss_list,'-',label ='train loss')\n",
    "    #plt.plot(range(1,ep+1),test_loss_list,'-',label ='test loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.ylim(0,1)\n",
    "    plt.savefig('./'+plot_name)\n",
    "\n",
    "def init_W2():\n",
    "    d = 784\n",
    "    h0 = 32\n",
    "    h1 = 32\n",
    "    k = 10\n",
    "    alpha1 = (1/d)**0.5\n",
    "    alpha2 = (1/h0)**0.5\n",
    "    alpha3 = (1/h1)**0.5\n",
    "    W_0 = np.random.uniform(-alpha1, alpha1,(h0,d))\n",
    "    W_1 = np.random.uniform(-alpha2, alpha2,(h1,h0))\n",
    "    W_2 = np.random.uniform(-alpha3, alpha3,(k,h1))\n",
    "    b_0 = np.zeros((h0,1))\n",
    "    b_1 = np.zeros((h1,1))\n",
    "    b_2 = np.zeros((k,1))\n",
    "    W_0 = torch.FloatTensor(W_0).requires_grad_(True)\n",
    "    W_1 = torch.FloatTensor(W_1).requires_grad_(True)\n",
    "    W_2 = torch.FloatTensor(W_2).requires_grad_(True)\n",
    "    b_2 = torch.FloatTensor(b_2).requires_grad_(True)\n",
    "    b_0 = torch.FloatTensor(b_0).requires_grad_(True)\n",
    "    b_1 = torch.FloatTensor(b_1).requires_grad_(True)\n",
    "\n",
    "    return W_0,W_1,W_2,b_0,b_1,b_2\n",
    "\n",
    "def fit_gd_cross_entropy2(x_train, y_train,x_test,y_test):\n",
    "    W_0,W_1,W_2,b_0,b_1,b_2 = init_W2()\n",
    "    loss_prev = 0\n",
    "    d_loss = 1\n",
    "    loss = 1\n",
    "    accuracy = 0\n",
    "    ep = 0\n",
    "    batchsize = 1000\n",
    "    nbatch = int(x_train.shape[0]/batchsize)\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    traccuracy = 0\n",
    "    taccuracy = 0\n",
    "    while traccuracy <0.99:\n",
    "        ep = ep+1\n",
    "        accuracy_list = []\n",
    "        for i in range(0, nbatch):\n",
    "            x = x_train[i*batchsize:(i+1)*batchsize]\n",
    "            y = y_train[i*batchsize:(i+1)*batchsize]\n",
    "            optim = torch.optim.Adam([W_0,W_1,b_0,b_1], lr=1e-2)\n",
    "            x_1 = torch.matmul(W_0, x.T) + b_0\n",
    "            x_1 = torch.nn.functional.relu(x_1)\n",
    "            x_2 = torch.matmul(W_1, x_1) + b_1\n",
    "            x_2 = torch.nn.functional.relu(x_2)\n",
    "            y_hat = torch.matmul(W_2, x_2) + b_2\n",
    "            #y_hat = torch.nn.functional.relu(y_hat)\n",
    "            y_hat = y_hat.T\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y.long())\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            accuracy = float(torch.sum(torch.argmax(y_hat, axis=1) == y)) / float(y.shape[0])\n",
    "            #accuracy_list.append(accuracy)\n",
    "            optim.step()\n",
    "        xtr_1 = torch.matmul(W_0, x_train.T) + b_0\n",
    "        xtr_1 = torch.nn.functional.relu(xtr_1)\n",
    "        xtr_2 = torch.matmul(W_1, xtr_1) + b_1\n",
    "        xtr_2 = torch.nn.functional.relu(xtr_2)\n",
    "        ytr_hat = torch.matmul(W_2, xtr_2) + b_2\n",
    "        #yt_hat = torch.nn.functional.relu(yt_hat)\n",
    "        ytr_hat = ytr_hat.T\n",
    "        train_loss = torch.nn.functional.cross_entropy(ytr_hat, y_train.long())\n",
    "        traccuracy = float(torch.sum(torch.argmax(ytr_hat, axis=1) == y_train)) / float(y_train.shape[0])\n",
    "        xt_1 = torch.matmul(W_0, x_test.T) + b_0\n",
    "        xt_1 = torch.nn.functional.relu(xt_1)\n",
    "        xt_2 = torch.matmul(W_1, xt_1) + b_1\n",
    "        xt_2 = torch.nn.functional.relu(xt_2)\n",
    "        yt_hat = torch.matmul(W_2, xt_2) + b_2\n",
    "        #yt_hat = torch.nn.functional.relu(yt_hat)\n",
    "        yt_hat = yt_hat.T\n",
    "        test_loss = torch.nn.functional.cross_entropy(yt_hat, y_test.long())\n",
    "        taccuracy = float(torch.sum(torch.argmax(yt_hat, axis=1) == y_test)) / float(y_test.shape[0])\n",
    "        train_accuracy_list.append(traccuracy)\n",
    "        test_accuracy_list.append(taccuracy)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        print(\" epoch {}, accurracy on training dataset {}, accuracy on test dataset {}; loss on training {} loss on test{} \".format(ep,traccuracy,taccuracy,train_loss,test_loss))\n",
    "        #print(\"epoch {}, accurracy on training dataset{}, accuracy on test dataset{} \".format(ep,traccuracy,taccuracy))\n",
    "    #print(\"Finally epoch {}, accurracy on training dataset{}, accuracy on test dataset{} \".format(ep,traccuracy,taccuracy))\n",
    "    print(\"Finally epoch {}, accurracy on training dataset {}, accuracy on test dataset {}; loss on training {} loss on test{} \".format(ep,traccuracy,taccuracy,train_loss,test_loss))\n",
    "    return(train_accuracy_list, test_accuracy_list,ep,train_loss_list,test_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_list, test_accuracy_list,ep,train_loss_list,test_loss_list = fit_gd_cross_entropy(x_train, y_train,x_test,y_test)\n",
    "plot(train_loss_list,ep,'a51.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 1, accurracy on training dataset 0.8062333333333334, accuracy on test dataset 0.8111; loss on training 0.6269136071205139 loss on test0.6178123950958252 \n",
      " epoch 2, accurracy on training dataset 0.8828333333333334, accuracy on test dataset 0.8873; loss on training 0.38648170232772827 loss on test0.3818037211894989 \n",
      " epoch 3, accurracy on training dataset 0.89855, accuracy on test dataset 0.8991; loss on training 0.3175697326660156 loss on test0.3184337317943573 \n",
      " epoch 4, accurracy on training dataset 0.9270666666666667, accuracy on test dataset 0.9265; loss on training 0.24751730263233185 loss on test0.2501721978187561 \n",
      " epoch 5, accurracy on training dataset 0.9353166666666667, accuracy on test dataset 0.933; loss on training 0.2148227095603943 loss on test0.22233837842941284 \n",
      " epoch 6, accurracy on training dataset 0.9432333333333334, accuracy on test dataset 0.9391; loss on training 0.18918219208717346 loss on test0.1981620043516159 \n",
      " epoch 7, accurracy on training dataset 0.9487333333333333, accuracy on test dataset 0.9428; loss on training 0.17165519297122955 loss on test0.18709832429885864 \n",
      " epoch 8, accurracy on training dataset 0.9519833333333333, accuracy on test dataset 0.9461; loss on training 0.15931572020053864 loss on test0.1794898957014084 \n",
      " epoch 9, accurracy on training dataset 0.9573833333333334, accuracy on test dataset 0.9504; loss on training 0.14214347302913666 loss on test0.16538988053798676 \n",
      " epoch 10, accurracy on training dataset 0.9619, accuracy on test dataset 0.9556; loss on training 0.1290488839149475 loss on test0.15823768079280853 \n",
      " epoch 11, accurracy on training dataset 0.9594666666666667, accuracy on test dataset 0.9538; loss on training 0.13558028638362885 loss on test0.1688084900379181 \n",
      " epoch 12, accurracy on training dataset 0.96325, accuracy on test dataset 0.9562; loss on training 0.1252308040857315 loss on test0.15925176441669464 \n",
      " epoch 13, accurracy on training dataset 0.9657833333333333, accuracy on test dataset 0.9567; loss on training 0.11580399423837662 loss on test0.1532752513885498 \n",
      " epoch 14, accurracy on training dataset 0.96345, accuracy on test dataset 0.9556; loss on training 0.12435818463563919 loss on test0.16465215384960175 \n",
      " epoch 15, accurracy on training dataset 0.9671833333333333, accuracy on test dataset 0.9604; loss on training 0.11227552592754364 loss on test0.15331964194774628 \n",
      " epoch 16, accurracy on training dataset 0.9689333333333333, accuracy on test dataset 0.9613; loss on training 0.10658643394708633 loss on test0.14989379048347473 \n",
      " epoch 17, accurracy on training dataset 0.9692, accuracy on test dataset 0.9586; loss on training 0.10717478394508362 loss on test0.15662367641925812 \n",
      " epoch 18, accurracy on training dataset 0.9696166666666667, accuracy on test dataset 0.9609; loss on training 0.10333257913589478 loss on test0.15491335093975067 \n",
      " epoch 19, accurracy on training dataset 0.9708833333333333, accuracy on test dataset 0.9596; loss on training 0.10148030519485474 loss on test0.156629741191864 \n",
      " epoch 20, accurracy on training dataset 0.9725666666666667, accuracy on test dataset 0.9625; loss on training 0.09482733905315399 loss on test0.15247365832328796 \n",
      " epoch 21, accurracy on training dataset 0.9725833333333334, accuracy on test dataset 0.9594; loss on training 0.09607245028018951 loss on test0.15652358531951904 \n",
      " epoch 22, accurracy on training dataset 0.9731333333333333, accuracy on test dataset 0.9608; loss on training 0.09267605096101761 loss on test0.15490932762622833 \n",
      " epoch 23, accurracy on training dataset 0.9733166666666667, accuracy on test dataset 0.9603; loss on training 0.09210625290870667 loss on test0.15530171990394592 \n",
      " epoch 24, accurracy on training dataset 0.9744666666666667, accuracy on test dataset 0.9604; loss on training 0.08725706487894058 loss on test0.15447506308555603 \n",
      " epoch 25, accurracy on training dataset 0.97585, accuracy on test dataset 0.9607; loss on training 0.0856451764702797 loss on test0.15577873587608337 \n",
      " epoch 26, accurracy on training dataset 0.9760166666666666, accuracy on test dataset 0.9606; loss on training 0.08535778522491455 loss on test0.15856610238552094 \n",
      " epoch 27, accurracy on training dataset 0.9756333333333334, accuracy on test dataset 0.9602; loss on training 0.08687613904476166 loss on test0.16351169347763062 \n",
      " epoch 28, accurracy on training dataset 0.9734, accuracy on test dataset 0.9569; loss on training 0.09276799857616425 loss on test0.17477142810821533 \n",
      " epoch 29, accurracy on training dataset 0.97305, accuracy on test dataset 0.9575; loss on training 0.09439519047737122 loss on test0.1808362603187561 \n",
      " epoch 30, accurracy on training dataset 0.9767, accuracy on test dataset 0.9597; loss on training 0.08358034491539001 loss on test0.1732088327407837 \n",
      " epoch 31, accurracy on training dataset 0.9725666666666667, accuracy on test dataset 0.9542; loss on training 0.09654204547405243 loss on test0.19395564496517181 \n",
      " epoch 32, accurracy on training dataset 0.9713666666666667, accuracy on test dataset 0.9544; loss on training 0.1022479310631752 loss on test0.20057068765163422 \n",
      " epoch 33, accurracy on training dataset 0.97695, accuracy on test dataset 0.9581; loss on training 0.08186955004930496 loss on test0.17982199788093567 \n",
      " epoch 34, accurracy on training dataset 0.97565, accuracy on test dataset 0.9571; loss on training 0.08601809293031693 loss on test0.1914597898721695 \n",
      " epoch 35, accurracy on training dataset 0.9713666666666667, accuracy on test dataset 0.9546; loss on training 0.10188480466604233 loss on test0.21147097647190094 \n",
      " epoch 36, accurracy on training dataset 0.9703666666666667, accuracy on test dataset 0.954; loss on training 0.10909934341907501 loss on test0.2233121544122696 \n",
      " epoch 37, accurracy on training dataset 0.9723333333333334, accuracy on test dataset 0.9554; loss on training 0.09874159097671509 loss on test0.21473461389541626 \n",
      " epoch 38, accurracy on training dataset 0.96615, accuracy on test dataset 0.9489; loss on training 0.12206639349460602 loss on test0.2431713342666626 \n",
      " epoch 39, accurracy on training dataset 0.9737666666666667, accuracy on test dataset 0.9558; loss on training 0.09529604017734528 loss on test0.22085288166999817 \n",
      " epoch 40, accurracy on training dataset 0.9740333333333333, accuracy on test dataset 0.954; loss on training 0.0932018905878067 loss on test0.22612975537776947 \n",
      " epoch 41, accurracy on training dataset 0.9688833333333333, accuracy on test dataset 0.9497; loss on training 0.11180897057056427 loss on test0.24869096279144287 \n",
      " epoch 42, accurracy on training dataset 0.97055, accuracy on test dataset 0.9503; loss on training 0.10448690503835678 loss on test0.24775362014770508 \n",
      " epoch 43, accurracy on training dataset 0.9734333333333334, accuracy on test dataset 0.9521; loss on training 0.09684859961271286 loss on test0.24223671853542328 \n",
      " epoch 44, accurracy on training dataset 0.9715333333333334, accuracy on test dataset 0.9492; loss on training 0.10426457971334457 loss on test0.26057347655296326 \n",
      " epoch 45, accurracy on training dataset 0.9725666666666667, accuracy on test dataset 0.9517; loss on training 0.09786882996559143 loss on test0.25059473514556885 \n",
      " epoch 46, accurracy on training dataset 0.9764, accuracy on test dataset 0.9538; loss on training 0.08624733984470367 loss on test0.23820924758911133 \n",
      " epoch 47, accurracy on training dataset 0.9734, accuracy on test dataset 0.9529; loss on training 0.0979560986161232 loss on test0.24916701018810272 \n",
      " epoch 48, accurracy on training dataset 0.9787166666666667, accuracy on test dataset 0.9575; loss on training 0.08129793405532837 loss on test0.2359187752008438 \n",
      " epoch 49, accurracy on training dataset 0.9781333333333333, accuracy on test dataset 0.9549; loss on training 0.0832277238368988 loss on test0.24282251298427582 \n",
      " epoch 50, accurracy on training dataset 0.9794666666666667, accuracy on test dataset 0.9569; loss on training 0.07935454696416855 loss on test0.24095311760902405 \n",
      " epoch 51, accurracy on training dataset 0.9766, accuracy on test dataset 0.9531; loss on training 0.0869419276714325 loss on test0.2559751272201538 \n",
      " epoch 52, accurracy on training dataset 0.9768333333333333, accuracy on test dataset 0.9526; loss on training 0.08923576027154922 loss on test0.2658091187477112 \n",
      " epoch 53, accurracy on training dataset 0.9781166666666666, accuracy on test dataset 0.9552; loss on training 0.08198383450508118 loss on test0.25530314445495605 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 54, accurracy on training dataset 0.9797833333333333, accuracy on test dataset 0.9559; loss on training 0.078181691467762 loss on test0.25317466259002686 \n",
      " epoch 55, accurracy on training dataset 0.97375, accuracy on test dataset 0.9512; loss on training 0.10031066834926605 loss on test0.2923891246318817 \n",
      " epoch 56, accurracy on training dataset 0.9747666666666667, accuracy on test dataset 0.9506; loss on training 0.09545282274484634 loss on test0.29304903745651245 \n",
      " epoch 57, accurracy on training dataset 0.9766833333333333, accuracy on test dataset 0.9526; loss on training 0.08973167091608047 loss on test0.2879754900932312 \n",
      " epoch 58, accurracy on training dataset 0.9757833333333333, accuracy on test dataset 0.9521; loss on training 0.09371744096279144 loss on test0.2933037579059601 \n",
      " epoch 59, accurracy on training dataset 0.9796166666666667, accuracy on test dataset 0.9556; loss on training 0.0798182263970375 loss on test0.26673558354377747 \n",
      " epoch 60, accurracy on training dataset 0.97625, accuracy on test dataset 0.9528; loss on training 0.08993463218212128 loss on test0.29008620977401733 \n",
      " epoch 61, accurracy on training dataset 0.9814166666666667, accuracy on test dataset 0.9559; loss on training 0.07127232849597931 loss on test0.2727184593677521 \n",
      " epoch 62, accurracy on training dataset 0.9808833333333333, accuracy on test dataset 0.9561; loss on training 0.07513107359409332 loss on test0.2764592468738556 \n",
      " epoch 63, accurracy on training dataset 0.9799666666666667, accuracy on test dataset 0.9549; loss on training 0.07948491722345352 loss on test0.2835414409637451 \n",
      " epoch 64, accurracy on training dataset 0.9807666666666667, accuracy on test dataset 0.9555; loss on training 0.07386776804924011 loss on test0.27968037128448486 \n",
      " epoch 65, accurracy on training dataset 0.9805666666666667, accuracy on test dataset 0.9553; loss on training 0.07703664898872375 loss on test0.28990691900253296 \n",
      " epoch 66, accurracy on training dataset 0.98135, accuracy on test dataset 0.9552; loss on training 0.07274990528821945 loss on test0.2894454896450043 \n",
      " epoch 67, accurracy on training dataset 0.9793, accuracy on test dataset 0.9536; loss on training 0.08563095331192017 loss on test0.30821168422698975 \n",
      " epoch 68, accurracy on training dataset 0.9814666666666667, accuracy on test dataset 0.9571; loss on training 0.07373116910457611 loss on test0.29095032811164856 \n",
      " epoch 69, accurracy on training dataset 0.9799666666666667, accuracy on test dataset 0.9554; loss on training 0.08218134939670563 loss on test0.308445543050766 \n",
      " epoch 70, accurracy on training dataset 0.9813666666666667, accuracy on test dataset 0.9573; loss on training 0.0776912122964859 loss on test0.3057694435119629 \n",
      " epoch 71, accurracy on training dataset 0.9797166666666667, accuracy on test dataset 0.9567; loss on training 0.084476999938488 loss on test0.3165920078754425 \n",
      " epoch 72, accurracy on training dataset 0.9802833333333333, accuracy on test dataset 0.9563; loss on training 0.07605724781751633 loss on test0.31979405879974365 \n",
      " epoch 73, accurracy on training dataset 0.9826666666666667, accuracy on test dataset 0.9588; loss on training 0.0713001936674118 loss on test0.31227362155914307 \n",
      " epoch 74, accurracy on training dataset 0.9810666666666666, accuracy on test dataset 0.9588; loss on training 0.07399974018335342 loss on test0.3175232708454132 \n",
      " epoch 75, accurracy on training dataset 0.9813666666666667, accuracy on test dataset 0.957; loss on training 0.07391421496868134 loss on test0.3175523281097412 \n",
      " epoch 76, accurracy on training dataset 0.98195, accuracy on test dataset 0.9579; loss on training 0.07244201749563217 loss on test0.32080739736557007 \n",
      " epoch 77, accurracy on training dataset 0.9792166666666666, accuracy on test dataset 0.9549; loss on training 0.08522546291351318 loss on test0.3363674581050873 \n",
      " epoch 78, accurracy on training dataset 0.97975, accuracy on test dataset 0.9564; loss on training 0.07861916720867157 loss on test0.3272850513458252 \n",
      " epoch 79, accurracy on training dataset 0.9832166666666666, accuracy on test dataset 0.9588; loss on training 0.06979023665189743 loss on test0.3181244134902954 \n",
      " epoch 80, accurracy on training dataset 0.98095, accuracy on test dataset 0.9567; loss on training 0.07937833666801453 loss on test0.33016565442085266 \n",
      " epoch 81, accurracy on training dataset 0.9811666666666666, accuracy on test dataset 0.9568; loss on training 0.07691074162721634 loss on test0.33141881227493286 \n",
      " epoch 82, accurracy on training dataset 0.9842833333333333, accuracy on test dataset 0.9586; loss on training 0.06620817631483078 loss on test0.3261759579181671 \n",
      " epoch 83, accurracy on training dataset 0.9827, accuracy on test dataset 0.9576; loss on training 0.07262410968542099 loss on test0.3315575122833252 \n",
      " epoch 84, accurracy on training dataset 0.9825833333333334, accuracy on test dataset 0.9563; loss on training 0.07342655956745148 loss on test0.339325875043869 \n",
      " epoch 85, accurracy on training dataset 0.9835166666666667, accuracy on test dataset 0.9567; loss on training 0.07226870208978653 loss on test0.34069010615348816 \n",
      " epoch 86, accurracy on training dataset 0.9847166666666667, accuracy on test dataset 0.9579; loss on training 0.06466840952634811 loss on test0.3308313190937042 \n",
      " epoch 87, accurracy on training dataset 0.9769333333333333, accuracy on test dataset 0.9551; loss on training 0.10348378121852875 loss on test0.3806105852127075 \n",
      " epoch 88, accurracy on training dataset 0.9817, accuracy on test dataset 0.9557; loss on training 0.07905744016170502 loss on test0.3562285304069519 \n",
      " epoch 89, accurracy on training dataset 0.9809333333333333, accuracy on test dataset 0.9546; loss on training 0.08245231211185455 loss on test0.36590123176574707 \n",
      " epoch 90, accurracy on training dataset 0.9779333333333333, accuracy on test dataset 0.9525; loss on training 0.09819385409355164 loss on test0.39486458897590637 \n",
      " epoch 91, accurracy on training dataset 0.9821666666666666, accuracy on test dataset 0.9551; loss on training 0.07775550335645676 loss on test0.3717973530292511 \n",
      " epoch 92, accurracy on training dataset 0.9800166666666666, accuracy on test dataset 0.9547; loss on training 0.08546989411115646 loss on test0.38834935426712036 \n",
      " epoch 93, accurracy on training dataset 0.9833, accuracy on test dataset 0.9555; loss on training 0.07219193875789642 loss on test0.3753434717655182 \n",
      " epoch 94, accurracy on training dataset 0.98145, accuracy on test dataset 0.9545; loss on training 0.08336088061332703 loss on test0.39491739869117737 \n",
      " epoch 95, accurracy on training dataset 0.9819333333333333, accuracy on test dataset 0.9545; loss on training 0.08233535289764404 loss on test0.38993528485298157 \n",
      " epoch 96, accurracy on training dataset 0.9844666666666667, accuracy on test dataset 0.9578; loss on training 0.07333292812108994 loss on test0.38144320249557495 \n",
      " epoch 97, accurracy on training dataset 0.9831833333333333, accuracy on test dataset 0.9552; loss on training 0.07522763311862946 loss on test0.3898047208786011 \n",
      " epoch 98, accurracy on training dataset 0.9826833333333334, accuracy on test dataset 0.9554; loss on training 0.07932838052511215 loss on test0.39098209142684937 \n",
      " epoch 99, accurracy on training dataset 0.9839166666666667, accuracy on test dataset 0.9581; loss on training 0.07440081983804703 loss on test0.3812430202960968 \n",
      " epoch 100, accurracy on training dataset 0.98365, accuracy on test dataset 0.9557; loss on training 0.07708468288183212 loss on test0.39811280369758606 \n",
      " epoch 101, accurracy on training dataset 0.9852, accuracy on test dataset 0.9571; loss on training 0.07102425396442413 loss on test0.39748942852020264 \n",
      " epoch 102, accurracy on training dataset 0.9819833333333333, accuracy on test dataset 0.9549; loss on training 0.086886465549469 loss on test0.41329407691955566 \n",
      " epoch 103, accurracy on training dataset 0.9817666666666667, accuracy on test dataset 0.9535; loss on training 0.0829809308052063 loss on test0.41398221254348755 \n",
      " epoch 104, accurracy on training dataset 0.9824666666666667, accuracy on test dataset 0.9568; loss on training 0.08560755848884583 loss on test0.42055997252464294 \n",
      " epoch 105, accurracy on training dataset 0.9834666666666667, accuracy on test dataset 0.957; loss on training 0.07291215658187866 loss on test0.404176265001297 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 106, accurracy on training dataset 0.9821833333333333, accuracy on test dataset 0.956; loss on training 0.08354117721319199 loss on test0.42984798550605774 \n",
      " epoch 107, accurracy on training dataset 0.98215, accuracy on test dataset 0.956; loss on training 0.08536242693662643 loss on test0.4404140114784241 \n",
      " epoch 108, accurracy on training dataset 0.9829166666666667, accuracy on test dataset 0.9565; loss on training 0.08402208983898163 loss on test0.4402032196521759 \n",
      " epoch 109, accurracy on training dataset 0.9822, accuracy on test dataset 0.9552; loss on training 0.08902964740991592 loss on test0.4504562020301819 \n",
      " epoch 110, accurracy on training dataset 0.9815666666666667, accuracy on test dataset 0.955; loss on training 0.08583610504865646 loss on test0.444577157497406 \n",
      " epoch 111, accurracy on training dataset 0.9826166666666667, accuracy on test dataset 0.9553; loss on training 0.08418342471122742 loss on test0.44404277205467224 \n",
      " epoch 112, accurracy on training dataset 0.9851833333333333, accuracy on test dataset 0.9576; loss on training 0.07285023480653763 loss on test0.4367034137248993 \n",
      " epoch 113, accurracy on training dataset 0.9821833333333333, accuracy on test dataset 0.955; loss on training 0.09005940705537796 loss on test0.4710048735141754 \n",
      " epoch 114, accurracy on training dataset 0.9849, accuracy on test dataset 0.9575; loss on training 0.07419043779373169 loss on test0.4556083083152771 \n",
      " epoch 115, accurracy on training dataset 0.9833, accuracy on test dataset 0.9556; loss on training 0.08127359300851822 loss on test0.47470444440841675 \n",
      " epoch 116, accurracy on training dataset 0.9844666666666667, accuracy on test dataset 0.9561; loss on training 0.07154697924852371 loss on test0.4702792465686798 \n",
      " epoch 117, accurracy on training dataset 0.98485, accuracy on test dataset 0.9569; loss on training 0.07371680438518524 loss on test0.47285687923431396 \n",
      " epoch 118, accurracy on training dataset 0.9856, accuracy on test dataset 0.9571; loss on training 0.0683874860405922 loss on test0.4726957082748413 \n",
      " epoch 119, accurracy on training dataset 0.9861, accuracy on test dataset 0.9575; loss on training 0.06709424406290054 loss on test0.46777603030204773 \n",
      " epoch 120, accurracy on training dataset 0.98305, accuracy on test dataset 0.9569; loss on training 0.08523865044116974 loss on test0.4965902864933014 \n",
      " epoch 121, accurracy on training dataset 0.9849166666666667, accuracy on test dataset 0.9578; loss on training 0.07426479458808899 loss on test0.479984849691391 \n",
      " epoch 122, accurracy on training dataset 0.9823833333333334, accuracy on test dataset 0.9566; loss on training 0.08551451563835144 loss on test0.5056639909744263 \n",
      " epoch 123, accurracy on training dataset 0.9803833333333334, accuracy on test dataset 0.9537; loss on training 0.10204713046550751 loss on test0.538409411907196 \n",
      " epoch 124, accurracy on training dataset 0.9843333333333333, accuracy on test dataset 0.9561; loss on training 0.08385971188545227 loss on test0.5167508721351624 \n",
      " epoch 125, accurracy on training dataset 0.98615, accuracy on test dataset 0.9578; loss on training 0.06920431554317474 loss on test0.49957558512687683 \n",
      " epoch 126, accurracy on training dataset 0.9827833333333333, accuracy on test dataset 0.9559; loss on training 0.08805372565984726 loss on test0.5191763043403625 \n",
      " epoch 127, accurracy on training dataset 0.9836166666666667, accuracy on test dataset 0.9574; loss on training 0.08321793377399445 loss on test0.5331916809082031 \n",
      " epoch 128, accurracy on training dataset 0.98515, accuracy on test dataset 0.9576; loss on training 0.07437261193990707 loss on test0.5290946364402771 \n",
      " epoch 129, accurracy on training dataset 0.984, accuracy on test dataset 0.9556; loss on training 0.08450116217136383 loss on test0.5392174124717712 \n",
      " epoch 130, accurracy on training dataset 0.9816166666666667, accuracy on test dataset 0.9552; loss on training 0.09974455833435059 loss on test0.5732576847076416 \n",
      " epoch 131, accurracy on training dataset 0.9848833333333333, accuracy on test dataset 0.9554; loss on training 0.07600029557943344 loss on test0.5458185076713562 \n",
      " epoch 132, accurracy on training dataset 0.9863666666666666, accuracy on test dataset 0.9576; loss on training 0.07162075489759445 loss on test0.5473299026489258 \n",
      " epoch 133, accurracy on training dataset 0.9851333333333333, accuracy on test dataset 0.9559; loss on training 0.07487396895885468 loss on test0.5541715621948242 \n",
      " epoch 134, accurracy on training dataset 0.9851, accuracy on test dataset 0.9559; loss on training 0.07758408784866333 loss on test0.5800840854644775 \n",
      " epoch 135, accurracy on training dataset 0.9848333333333333, accuracy on test dataset 0.9554; loss on training 0.07981622219085693 loss on test0.5803945660591125 \n",
      " epoch 136, accurracy on training dataset 0.9859333333333333, accuracy on test dataset 0.9564; loss on training 0.07317262887954712 loss on test0.5677999258041382 \n",
      " epoch 137, accurracy on training dataset 0.9861, accuracy on test dataset 0.9561; loss on training 0.07472260296344757 loss on test0.5840174555778503 \n",
      " epoch 138, accurracy on training dataset 0.9837166666666667, accuracy on test dataset 0.9537; loss on training 0.08205194026231766 loss on test0.5942633152008057 \n",
      " epoch 139, accurracy on training dataset 0.9859, accuracy on test dataset 0.9551; loss on training 0.073668472468853 loss on test0.6084690690040588 \n",
      " epoch 140, accurracy on training dataset 0.9848833333333333, accuracy on test dataset 0.9547; loss on training 0.07562382519245148 loss on test0.5969368815422058 \n",
      " epoch 141, accurracy on training dataset 0.9852, accuracy on test dataset 0.9552; loss on training 0.07763838768005371 loss on test0.6188457608222961 \n",
      " epoch 142, accurracy on training dataset 0.9846, accuracy on test dataset 0.9556; loss on training 0.08121044933795929 loss on test0.610781192779541 \n",
      " epoch 143, accurracy on training dataset 0.9829, accuracy on test dataset 0.9538; loss on training 0.08753690868616104 loss on test0.6288958787918091 \n",
      " epoch 144, accurracy on training dataset 0.9838833333333333, accuracy on test dataset 0.9533; loss on training 0.08563396334648132 loss on test0.6377405524253845 \n",
      " epoch 145, accurracy on training dataset 0.98375, accuracy on test dataset 0.9539; loss on training 0.08951428532600403 loss on test0.6430334448814392 \n",
      " epoch 146, accurracy on training dataset 0.9858666666666667, accuracy on test dataset 0.9554; loss on training 0.07129278779029846 loss on test0.6179172992706299 \n",
      " epoch 147, accurracy on training dataset 0.9849, accuracy on test dataset 0.9544; loss on training 0.08058799058198929 loss on test0.6454436182975769 \n",
      " epoch 148, accurracy on training dataset 0.98245, accuracy on test dataset 0.9541; loss on training 0.09635908156633377 loss on test0.652489185333252 \n",
      " epoch 149, accurracy on training dataset 0.9857666666666667, accuracy on test dataset 0.9558; loss on training 0.07689392566680908 loss on test0.651175856590271 \n",
      " epoch 150, accurracy on training dataset 0.9841166666666666, accuracy on test dataset 0.9559; loss on training 0.0845237746834755 loss on test0.6571683883666992 \n",
      " epoch 151, accurracy on training dataset 0.9843, accuracy on test dataset 0.9557; loss on training 0.09162317216396332 loss on test0.6632319092750549 \n",
      " epoch 152, accurracy on training dataset 0.9845, accuracy on test dataset 0.954; loss on training 0.08661137521266937 loss on test0.6808741688728333 \n",
      " epoch 153, accurracy on training dataset 0.9851, accuracy on test dataset 0.9546; loss on training 0.0825733095407486 loss on test0.6802215576171875 \n",
      " epoch 154, accurracy on training dataset 0.9834666666666667, accuracy on test dataset 0.9548; loss on training 0.09833645820617676 loss on test0.6905359625816345 \n",
      " epoch 155, accurracy on training dataset 0.98345, accuracy on test dataset 0.9539; loss on training 0.09565823525190353 loss on test0.695728063583374 \n",
      " epoch 156, accurracy on training dataset 0.9842833333333333, accuracy on test dataset 0.9547; loss on training 0.08560764789581299 loss on test0.6893071532249451 \n",
      " epoch 157, accurracy on training dataset 0.9832666666666666, accuracy on test dataset 0.9542; loss on training 0.09526078402996063 loss on test0.7089862823486328 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 158, accurracy on training dataset 0.9833666666666666, accuracy on test dataset 0.9546; loss on training 0.09181675314903259 loss on test0.7045813798904419 \n",
      " epoch 159, accurracy on training dataset 0.9858, accuracy on test dataset 0.9545; loss on training 0.07795766741037369 loss on test0.7000687718391418 \n",
      " epoch 160, accurracy on training dataset 0.98455, accuracy on test dataset 0.9547; loss on training 0.0867539793252945 loss on test0.6984108686447144 \n",
      " epoch 161, accurracy on training dataset 0.9867, accuracy on test dataset 0.9556; loss on training 0.07470854371786118 loss on test0.6954549551010132 \n",
      " epoch 162, accurracy on training dataset 0.9850666666666666, accuracy on test dataset 0.9559; loss on training 0.0852050632238388 loss on test0.7001254558563232 \n",
      " epoch 163, accurracy on training dataset 0.9857333333333334, accuracy on test dataset 0.9554; loss on training 0.07715557515621185 loss on test0.7079327702522278 \n",
      " epoch 164, accurracy on training dataset 0.9873166666666666, accuracy on test dataset 0.9554; loss on training 0.06756247580051422 loss on test0.7011417746543884 \n",
      " epoch 165, accurracy on training dataset 0.9868666666666667, accuracy on test dataset 0.9556; loss on training 0.07198943942785263 loss on test0.7026603817939758 \n",
      " epoch 166, accurracy on training dataset 0.98475, accuracy on test dataset 0.955; loss on training 0.08753735572099686 loss on test0.7243751883506775 \n",
      " epoch 167, accurracy on training dataset 0.9847333333333333, accuracy on test dataset 0.9559; loss on training 0.08804716169834137 loss on test0.7119695544242859 \n",
      " epoch 168, accurracy on training dataset 0.9857833333333333, accuracy on test dataset 0.9572; loss on training 0.07670517265796661 loss on test0.7145119905471802 \n",
      " epoch 169, accurracy on training dataset 0.9865, accuracy on test dataset 0.9561; loss on training 0.07640974223613739 loss on test0.7211819887161255 \n",
      " epoch 170, accurracy on training dataset 0.9871333333333333, accuracy on test dataset 0.9568; loss on training 0.06949684768915176 loss on test0.7221502661705017 \n",
      " epoch 171, accurracy on training dataset 0.98485, accuracy on test dataset 0.9558; loss on training 0.08293194323778152 loss on test0.745601236820221 \n",
      " epoch 172, accurracy on training dataset 0.9892, accuracy on test dataset 0.9579; loss on training 0.05853535607457161 loss on test0.7145671248435974 \n",
      " epoch 173, accurracy on training dataset 0.98875, accuracy on test dataset 0.9584; loss on training 0.06526540219783783 loss on test0.7431364059448242 \n",
      " epoch 174, accurracy on training dataset 0.98805, accuracy on test dataset 0.9568; loss on training 0.06806021928787231 loss on test0.7493711709976196 \n",
      " epoch 175, accurracy on training dataset 0.9885833333333334, accuracy on test dataset 0.9592; loss on training 0.062374457716941833 loss on test0.7458603382110596 \n",
      " epoch 176, accurracy on training dataset 0.9868, accuracy on test dataset 0.9567; loss on training 0.07057807594537735 loss on test0.7515019774436951 \n",
      " epoch 177, accurracy on training dataset 0.9889166666666667, accuracy on test dataset 0.9574; loss on training 0.059519365429878235 loss on test0.7249089479446411 \n",
      " epoch 178, accurracy on training dataset 0.98835, accuracy on test dataset 0.9565; loss on training 0.06189125031232834 loss on test0.7450636029243469 \n",
      " epoch 179, accurracy on training dataset 0.9888166666666667, accuracy on test dataset 0.9566; loss on training 0.06764712929725647 loss on test0.7493982315063477 \n",
      " epoch 180, accurracy on training dataset 0.9867833333333333, accuracy on test dataset 0.9559; loss on training 0.07647918909788132 loss on test0.7819249033927917 \n",
      " epoch 181, accurracy on training dataset 0.98515, accuracy on test dataset 0.9548; loss on training 0.09061891585588455 loss on test0.7984433174133301 \n",
      " epoch 182, accurracy on training dataset 0.9885333333333334, accuracy on test dataset 0.9573; loss on training 0.06260356307029724 loss on test0.7767751812934875 \n",
      " epoch 183, accurracy on training dataset 0.9855833333333334, accuracy on test dataset 0.9555; loss on training 0.08317326754331589 loss on test0.8146910071372986 \n",
      " epoch 184, accurracy on training dataset 0.9877333333333334, accuracy on test dataset 0.9567; loss on training 0.07483435422182083 loss on test0.79449462890625 \n",
      " epoch 185, accurracy on training dataset 0.9863, accuracy on test dataset 0.9563; loss on training 0.08366546779870987 loss on test0.804314911365509 \n",
      " epoch 186, accurracy on training dataset 0.9873, accuracy on test dataset 0.9558; loss on training 0.07762745767831802 loss on test0.8128495812416077 \n",
      " epoch 187, accurracy on training dataset 0.9863333333333333, accuracy on test dataset 0.9561; loss on training 0.08352503180503845 loss on test0.8171346187591553 \n",
      " epoch 188, accurracy on training dataset 0.9882166666666666, accuracy on test dataset 0.9571; loss on training 0.06708943098783493 loss on test0.8071006536483765 \n",
      " epoch 189, accurracy on training dataset 0.9859, accuracy on test dataset 0.9552; loss on training 0.09177445620298386 loss on test0.8454002737998962 \n",
      " epoch 190, accurracy on training dataset 0.9859, accuracy on test dataset 0.9566; loss on training 0.09721606969833374 loss on test0.8548023700714111 \n",
      " epoch 191, accurracy on training dataset 0.9853833333333334, accuracy on test dataset 0.9555; loss on training 0.09844087064266205 loss on test0.8358816504478455 \n",
      " epoch 192, accurracy on training dataset 0.9870833333333333, accuracy on test dataset 0.9576; loss on training 0.08106779307126999 loss on test0.8416599631309509 \n",
      " epoch 193, accurracy on training dataset 0.9849833333333333, accuracy on test dataset 0.9559; loss on training 0.0940115675330162 loss on test0.8636038303375244 \n",
      " epoch 194, accurracy on training dataset 0.9873833333333333, accuracy on test dataset 0.9577; loss on training 0.0802205502986908 loss on test0.8468520641326904 \n",
      " epoch 195, accurracy on training dataset 0.9876166666666667, accuracy on test dataset 0.9572; loss on training 0.07383021712303162 loss on test0.8516949415206909 \n",
      " epoch 196, accurracy on training dataset 0.9885833333333334, accuracy on test dataset 0.9561; loss on training 0.06823389232158661 loss on test0.8560079336166382 \n",
      " epoch 197, accurracy on training dataset 0.9874666666666667, accuracy on test dataset 0.9573; loss on training 0.08659588545560837 loss on test0.8645173907279968 \n",
      " epoch 198, accurracy on training dataset 0.9896166666666667, accuracy on test dataset 0.9578; loss on training 0.061907362192869186 loss on test0.8363384008407593 \n",
      " epoch 199, accurracy on training dataset 0.9882666666666666, accuracy on test dataset 0.9572; loss on training 0.0722019225358963 loss on test0.845772922039032 \n",
      " epoch 200, accurracy on training dataset 0.98765, accuracy on test dataset 0.9576; loss on training 0.08033280074596405 loss on test0.8825320601463318 \n",
      " epoch 201, accurracy on training dataset 0.9884833333333334, accuracy on test dataset 0.9576; loss on training 0.0733000859618187 loss on test0.8827829957008362 \n",
      " epoch 202, accurracy on training dataset 0.9890166666666667, accuracy on test dataset 0.9577; loss on training 0.06168122589588165 loss on test0.8851321339607239 \n",
      " epoch 203, accurracy on training dataset 0.9874833333333334, accuracy on test dataset 0.9564; loss on training 0.07456199824810028 loss on test0.9138029217720032 \n",
      " epoch 204, accurracy on training dataset 0.9885666666666667, accuracy on test dataset 0.9567; loss on training 0.07212934643030167 loss on test0.9017614126205444 \n",
      " epoch 205, accurracy on training dataset 0.9882, accuracy on test dataset 0.9562; loss on training 0.07112003862857819 loss on test0.9245002865791321 \n",
      " epoch 206, accurracy on training dataset 0.9893166666666666, accuracy on test dataset 0.9575; loss on training 0.06779590249061584 loss on test0.9184325337409973 \n",
      " epoch 207, accurracy on training dataset 0.9900166666666667, accuracy on test dataset 0.9584; loss on training 0.061322323977947235 loss on test0.9186818599700928 \n",
      "Finally epoch 207, accurracy on training dataset 0.9900166666666667, accuracy on test dataset 0.9584; loss on training 0.061322323977947235 loss on test0.9186818599700928 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accuracy_list, test_accuracy_list,ep,train_loss_list,test_loss_list = fit_gd_cross_entropy2(x_train, y_train,x_test,y_test)\n",
    "plot(train_loss_list,ep,'a52.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
