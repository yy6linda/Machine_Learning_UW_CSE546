{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "mndata = MNIST('./data/')\n",
    "x_train, y_train = map(np.array, mndata.load_training())\n",
    "x_test, y_test = map(np.array, mndata.load_testing())\n",
    "x_train = torch.FloatTensor(x_train/255.0)\n",
    "y_train = torch.tensor(y_train)\n",
    "x_test = torch.FloatTensor(x_test/255.0)\n",
    "y_test = torch.tensor(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W():\n",
    "    d = 784\n",
    "    h = 64\n",
    "    k = 10\n",
    "    alpha1 = (1/d)**0.5\n",
    "    alpha2 = (1/h)**0.5\n",
    "    W_0 = np.random.uniform(-alpha1, alpha1,(h,d))\n",
    "    W_1 = np.random.uniform(-alpha2, alpha2,(k,h))\n",
    "    b_0 = np.zeros((h,1))\n",
    "    b_1 = np.zeros((k,1))\n",
    "    W_0 = torch.FloatTensor(W_0).requires_grad_(True)\n",
    "    W_1 = torch.FloatTensor(W_1).requires_grad_(True)\n",
    "    b_0 = torch.FloatTensor(b_0).requires_grad_(True)\n",
    "    b_1 = torch.FloatTensor(b_1).requires_grad_(True)\n",
    "    return W_0,W_1,b_0,b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gd_cross_entropy(x_train, y_train,x_test,y_test):\n",
    "    W_0,W_1,b_0,b_1 = init_W()\n",
    "    loss_prev = 0\n",
    "    d_loss = 1\n",
    "    loss = 1\n",
    "    accuracy = 0\n",
    "    ep = 0\n",
    "    batchsize = 1000\n",
    "    nbatch = int(x_train.shape[0]/batchsize)\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    traccuracy = 0\n",
    "    taccuracy = 0\n",
    "    train_loss_list = []\n",
    "    while traccuracy <0.99:\n",
    "        ep = ep+1\n",
    "        accuracy_list = []\n",
    "        for i in range(0, nbatch):\n",
    "            x = x_train[i*batchsize:(i+1)*batchsize]\n",
    "            y = y_train[i*batchsize:(i+1)*batchsize]\n",
    "            optim = torch.optim.Adam([W_0,W_1,b_0,b_1], lr=1e-3)\n",
    "            x_1 = torch.matmul(W_0, x.T) + b_0\n",
    "            x_1 = torch.nn.functional.relu(x_1)\n",
    "            y_hat = torch.matmul(W_1, x_1) + b_1\n",
    "            #y_hat = torch.nn.functional.relu(y_hat)\n",
    "            y_hat = y_hat.T\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y.long())\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            accuracy = float(torch.sum(torch.argmax(y_hat, axis=1) == y)) / float(y.shape[0])\n",
    "            #accuracy_list.append(accuracy)\n",
    "            optim.step()\n",
    "        xtr_1 = torch.matmul(W_0, x_train.T) + b_0\n",
    "        xtr_1 = torch.nn.functional.relu(xtr_1)\n",
    "        ytr_hat = torch.matmul(W_1, xtr_1) + b_1\n",
    "        #yt_hat = torch.nn.functional.relu(yt_hat)\n",
    "        ytr_hat = ytr_hat.T\n",
    "        train_loss = torch.nn.functional.cross_entropy(ytr_hat, y_train.long())\n",
    "        train_loss_list.append(train_loss)\n",
    "        traccuracy = float(torch.sum(torch.argmax(ytr_hat, axis=1) == y_train)) / float(y_train.shape[0])\n",
    "        xt_1 = torch.matmul(W_0, x_test.T) + b_0\n",
    "        xt_1 = torch.nn.functional.relu(xt_1)\n",
    "        yt_hat = torch.matmul(W_1, xt_1) + b_1\n",
    "        #yt_hat = torch.nn.functional.relu(yt_hat)\n",
    "        yt_hat = yt_hat.T\n",
    "        test_loss = torch.nn.functional.cross_entropy(yt_hat, y_test.long())\n",
    "        test_loss_list.append(test_loss)\n",
    "        #print(test_loss)\n",
    "        taccuracy = float(torch.sum(torch.argmax(yt_hat, axis=1) == y_test)) / float(y_test.shape[0])\n",
    "        train_accuracy_list.append(traccuracy)\n",
    "        test_accuracy_list.append(taccuracy)\n",
    "        print(\"epoch {}, accurracy on training dataset{}, accuracy on test dataset{}， loss on training dataset{}, loss on test dataset{}  \".format(ep,traccuracy,taccuracy,train_loss,test_loss))    \n",
    "    print(\"Finally epoch {}, accurracy on training dataset{}, accuracy on test dataset{} \".format(ep,traccuracy,taccuracy))        \n",
    "    return(train_accuracy_list, test_accuracy_list,ep,train_loss_list,test_loss_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, accurracy on training dataset0.87245, accuracy on test dataset0.8805， loss on training dataset0.5077205300331116, loss on test dataset0.4903803765773773  \n",
      "epoch 2, accurracy on training dataset0.9024, accuracy on test dataset0.907， loss on training dataset0.35048845410346985, loss on test dataset0.3390342593193054  \n",
      "epoch 3, accurracy on training dataset0.9136166666666666, accuracy on test dataset0.9167， loss on training dataset0.30396756529808044, loss on test dataset0.2965088486671448  \n",
      "epoch 4, accurracy on training dataset0.9208666666666666, accuracy on test dataset0.9211， loss on training dataset0.2776532471179962, loss on test dataset0.2732560634613037  \n",
      "epoch 5, accurracy on training dataset0.9259666666666667, accuracy on test dataset0.9277， loss on training dataset0.25817638635635376, loss on test dataset0.2562885582447052  \n",
      "epoch 6, accurracy on training dataset0.93085, accuracy on test dataset0.9311， loss on training dataset0.24151943624019623, loss on test dataset0.24174797534942627  \n",
      "epoch 7, accurracy on training dataset0.9355833333333333, accuracy on test dataset0.9357， loss on training dataset0.2263735979795456, loss on test dataset0.22802963852882385  \n",
      "epoch 8, accurracy on training dataset0.93985, accuracy on test dataset0.9377， loss on training dataset0.2118910849094391, loss on test dataset0.21464575827121735  \n",
      "epoch 9, accurracy on training dataset0.9438166666666666, accuracy on test dataset0.9416， loss on training dataset0.19891385734081268, loss on test dataset0.20270544290542603  \n",
      "epoch 10, accurracy on training dataset0.9468833333333333, accuracy on test dataset0.9434， loss on training dataset0.18749447166919708, loss on test dataset0.19233101606369019  \n",
      "epoch 11, accurracy on training dataset0.9495666666666667, accuracy on test dataset0.9459， loss on training dataset0.17708294093608856, loss on test dataset0.18333803117275238  \n",
      "epoch 12, accurracy on training dataset0.9520166666666666, accuracy on test dataset0.9483， loss on training dataset0.1678074449300766, loss on test dataset0.17550671100616455  \n",
      "epoch 13, accurracy on training dataset0.95445, accuracy on test dataset0.9511， loss on training dataset0.15952342748641968, loss on test dataset0.16867844760417938  \n",
      "epoch 14, accurracy on training dataset0.9564833333333334, accuracy on test dataset0.9534， loss on training dataset0.15209150314331055, loss on test dataset0.1627838909626007  \n",
      "epoch 15, accurracy on training dataset0.9582666666666667, accuracy on test dataset0.9551， loss on training dataset0.14517709612846375, loss on test dataset0.15734706819057465  \n",
      "epoch 16, accurracy on training dataset0.96045, accuracy on test dataset0.9571， loss on training dataset0.1387280821800232, loss on test dataset0.15264679491519928  \n",
      "epoch 17, accurracy on training dataset0.96195, accuracy on test dataset0.9581， loss on training dataset0.13303598761558533, loss on test dataset0.14849959313869476  \n",
      "epoch 18, accurracy on training dataset0.9634666666666667, accuracy on test dataset0.9598， loss on training dataset0.12790946662425995, loss on test dataset0.14493438601493835  \n",
      "epoch 19, accurracy on training dataset0.9649833333333333, accuracy on test dataset0.9609， loss on training dataset0.12289991229772568, loss on test dataset0.14146780967712402  \n",
      "epoch 20, accurracy on training dataset0.9660333333333333, accuracy on test dataset0.9611， loss on training dataset0.1183817982673645, loss on test dataset0.13837513327598572  \n",
      "epoch 21, accurracy on training dataset0.9671333333333333, accuracy on test dataset0.9623， loss on training dataset0.11429587751626968, loss on test dataset0.1356184333562851  \n",
      "epoch 22, accurracy on training dataset0.96835, accuracy on test dataset0.9637， loss on training dataset0.11048921942710876, loss on test dataset0.13318073749542236  \n",
      "epoch 23, accurracy on training dataset0.9696, accuracy on test dataset0.9649， loss on training dataset0.10697606950998306, loss on test dataset0.1306561380624771  \n",
      "epoch 24, accurracy on training dataset0.9704333333333334, accuracy on test dataset0.9654， loss on training dataset0.10372183471918106, loss on test dataset0.12844696640968323  \n",
      "epoch 25, accurracy on training dataset0.9713666666666667, accuracy on test dataset0.9652， loss on training dataset0.1007869616150856, loss on test dataset0.1265287697315216  \n",
      "epoch 26, accurracy on training dataset0.9721666666666666, accuracy on test dataset0.9664， loss on training dataset0.09814446419477463, loss on test dataset0.12503661215305328  \n",
      "epoch 27, accurracy on training dataset0.9729, accuracy on test dataset0.9671， loss on training dataset0.09543677419424057, loss on test dataset0.12339916080236435  \n",
      "epoch 28, accurracy on training dataset0.9734666666666667, accuracy on test dataset0.9669， loss on training dataset0.09297560155391693, loss on test dataset0.1221439316868782  \n",
      "epoch 29, accurracy on training dataset0.974, accuracy on test dataset0.9671， loss on training dataset0.09058982878923416, loss on test dataset0.12099850177764893  \n",
      "epoch 30, accurracy on training dataset0.9747666666666667, accuracy on test dataset0.9674， loss on training dataset0.08847563713788986, loss on test dataset0.12045469880104065  \n",
      "epoch 31, accurracy on training dataset0.97545, accuracy on test dataset0.9678， loss on training dataset0.08636821061372757, loss on test dataset0.11984335631132126  \n",
      "epoch 32, accurracy on training dataset0.9762, accuracy on test dataset0.968， loss on training dataset0.08443708717823029, loss on test dataset0.11929000169038773  \n",
      "epoch 33, accurracy on training dataset0.9768, accuracy on test dataset0.9682， loss on training dataset0.08264435082674026, loss on test dataset0.11872592568397522  \n",
      "epoch 34, accurracy on training dataset0.9772833333333333, accuracy on test dataset0.9682， loss on training dataset0.08093657344579697, loss on test dataset0.11818193644285202  \n",
      "epoch 35, accurracy on training dataset0.9779, accuracy on test dataset0.9683， loss on training dataset0.07930111140012741, loss on test dataset0.11793848872184753  \n",
      "epoch 36, accurracy on training dataset0.9783666666666667, accuracy on test dataset0.9686， loss on training dataset0.0778142586350441, loss on test dataset0.11791757494211197  \n",
      "epoch 37, accurracy on training dataset0.9788666666666667, accuracy on test dataset0.9684， loss on training dataset0.0764002799987793, loss on test dataset0.11785253882408142  \n",
      "epoch 38, accurracy on training dataset0.9793333333333333, accuracy on test dataset0.9679， loss on training dataset0.07507799565792084, loss on test dataset0.11788967251777649  \n",
      "epoch 39, accurracy on training dataset0.9798333333333333, accuracy on test dataset0.9679， loss on training dataset0.07362672686576843, loss on test dataset0.11785650998353958  \n",
      "epoch 40, accurracy on training dataset0.9801166666666666, accuracy on test dataset0.9679， loss on training dataset0.0725245475769043, loss on test dataset0.11826913803815842  \n",
      "epoch 41, accurracy on training dataset0.9801666666666666, accuracy on test dataset0.9675， loss on training dataset0.07143668085336685, loss on test dataset0.1186395138502121  \n",
      "epoch 42, accurracy on training dataset0.9804666666666667, accuracy on test dataset0.9674， loss on training dataset0.07033172994852066, loss on test dataset0.11913134902715683  \n",
      "epoch 43, accurracy on training dataset0.9806666666666667, accuracy on test dataset0.9674， loss on training dataset0.06918139010667801, loss on test dataset0.11945836246013641  \n",
      "epoch 44, accurracy on training dataset0.9809166666666667, accuracy on test dataset0.968， loss on training dataset0.06813254952430725, loss on test dataset0.11969149112701416  \n",
      "epoch 45, accurracy on training dataset0.9810666666666666, accuracy on test dataset0.9676， loss on training dataset0.06711072474718094, loss on test dataset0.12013062834739685  \n",
      "epoch 46, accurracy on training dataset0.9815666666666667, accuracy on test dataset0.9681， loss on training dataset0.06598988175392151, loss on test dataset0.12043146044015884  \n",
      "epoch 47, accurracy on training dataset0.9818333333333333, accuracy on test dataset0.968， loss on training dataset0.06509817391633987, loss on test dataset0.12083166837692261  \n",
      "epoch 48, accurracy on training dataset0.9818833333333333, accuracy on test dataset0.9681， loss on training dataset0.06428978592157364, loss on test dataset0.12133980542421341  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49, accurracy on training dataset0.9823333333333333, accuracy on test dataset0.968， loss on training dataset0.06350728124380112, loss on test dataset0.12184586375951767  \n",
      "epoch 50, accurracy on training dataset0.9824333333333334, accuracy on test dataset0.9683， loss on training dataset0.0627574697136879, loss on test dataset0.122222900390625  \n",
      "epoch 51, accurracy on training dataset0.9827833333333333, accuracy on test dataset0.9681， loss on training dataset0.062033142894506454, loss on test dataset0.1226378083229065  \n",
      "epoch 52, accurracy on training dataset0.9828833333333333, accuracy on test dataset0.9679， loss on training dataset0.06132935360074043, loss on test dataset0.12300563603639603  \n",
      "epoch 53, accurracy on training dataset0.98335, accuracy on test dataset0.9684， loss on training dataset0.06055585667490959, loss on test dataset0.12334411591291428  \n",
      "epoch 54, accurracy on training dataset0.98355, accuracy on test dataset0.9685， loss on training dataset0.05984031409025192, loss on test dataset0.12396423518657684  \n",
      "epoch 55, accurracy on training dataset0.9838166666666667, accuracy on test dataset0.9682， loss on training dataset0.05907357484102249, loss on test dataset0.1242402121424675  \n",
      "epoch 56, accurracy on training dataset0.98395, accuracy on test dataset0.968， loss on training dataset0.05834876373410225, loss on test dataset0.12483331561088562  \n",
      "epoch 57, accurracy on training dataset0.98435, accuracy on test dataset0.9682， loss on training dataset0.057536832988262177, loss on test dataset0.1253269910812378  \n",
      "epoch 58, accurracy on training dataset0.9845333333333334, accuracy on test dataset0.9686， loss on training dataset0.056938961148262024, loss on test dataset0.1262345314025879  \n",
      "epoch 59, accurracy on training dataset0.9848833333333333, accuracy on test dataset0.9685， loss on training dataset0.05621026083827019, loss on test dataset0.12708333134651184  \n",
      "epoch 60, accurracy on training dataset0.98505, accuracy on test dataset0.9682， loss on training dataset0.05556309223175049, loss on test dataset0.1274816393852234  \n",
      "epoch 61, accurracy on training dataset0.9852833333333333, accuracy on test dataset0.9682， loss on training dataset0.0550113245844841, loss on test dataset0.128456711769104  \n",
      "epoch 62, accurracy on training dataset0.9853833333333334, accuracy on test dataset0.968， loss on training dataset0.054567791521549225, loss on test dataset0.12958350777626038  \n",
      "epoch 63, accurracy on training dataset0.98555, accuracy on test dataset0.9681， loss on training dataset0.053872883319854736, loss on test dataset0.1303092986345291  \n",
      "epoch 64, accurracy on training dataset0.9857333333333334, accuracy on test dataset0.9676， loss on training dataset0.053348153829574585, loss on test dataset0.13123300671577454  \n",
      "epoch 65, accurracy on training dataset0.9857833333333333, accuracy on test dataset0.9681， loss on training dataset0.05270301550626755, loss on test dataset0.13190913200378418  \n",
      "epoch 66, accurracy on training dataset0.9860833333333333, accuracy on test dataset0.9682， loss on training dataset0.05203190818428993, loss on test dataset0.1326708197593689  \n",
      "epoch 67, accurracy on training dataset0.9861166666666666, accuracy on test dataset0.9677， loss on training dataset0.05158069357275963, loss on test dataset0.13376474380493164  \n",
      "epoch 68, accurracy on training dataset0.9861, accuracy on test dataset0.9681， loss on training dataset0.05095567926764488, loss on test dataset0.134589284658432  \n",
      "epoch 69, accurracy on training dataset0.98615, accuracy on test dataset0.9682， loss on training dataset0.05045628920197487, loss on test dataset0.1351337432861328  \n",
      "epoch 70, accurracy on training dataset0.9864, accuracy on test dataset0.9685， loss on training dataset0.050051022320985794, loss on test dataset0.13651806116104126  \n",
      "epoch 71, accurracy on training dataset0.9865666666666667, accuracy on test dataset0.9687， loss on training dataset0.04968426376581192, loss on test dataset0.13771289587020874  \n",
      "epoch 72, accurracy on training dataset0.9866833333333334, accuracy on test dataset0.9689， loss on training dataset0.0493585467338562, loss on test dataset0.13875998556613922  \n",
      "epoch 73, accurracy on training dataset0.9867833333333333, accuracy on test dataset0.969， loss on training dataset0.04908236488699913, loss on test dataset0.13959017395973206  \n",
      "epoch 74, accurracy on training dataset0.9869666666666667, accuracy on test dataset0.9687， loss on training dataset0.04876299947500229, loss on test dataset0.14101259410381317  \n",
      "epoch 75, accurracy on training dataset0.98695, accuracy on test dataset0.9684， loss on training dataset0.04863884299993515, loss on test dataset0.14255499839782715  \n",
      "epoch 76, accurracy on training dataset0.9870333333333333, accuracy on test dataset0.9681， loss on training dataset0.04830372333526611, loss on test dataset0.1438988447189331  \n",
      "epoch 77, accurracy on training dataset0.9871333333333333, accuracy on test dataset0.9681， loss on training dataset0.0480109304189682, loss on test dataset0.14531826972961426  \n",
      "epoch 78, accurracy on training dataset0.9870333333333333, accuracy on test dataset0.9683， loss on training dataset0.04798702895641327, loss on test dataset0.1466020941734314  \n",
      "epoch 79, accurracy on training dataset0.9870333333333333, accuracy on test dataset0.9682， loss on training dataset0.047841109335422516, loss on test dataset0.1477005034685135  \n",
      "epoch 80, accurracy on training dataset0.9869833333333333, accuracy on test dataset0.968， loss on training dataset0.04765959829092026, loss on test dataset0.14905495941638947  \n",
      "epoch 81, accurracy on training dataset0.9869833333333333, accuracy on test dataset0.9677， loss on training dataset0.047413382679224014, loss on test dataset0.14994879066944122  \n",
      "epoch 82, accurracy on training dataset0.98725, accuracy on test dataset0.9678， loss on training dataset0.047097522765398026, loss on test dataset0.15086966753005981  \n",
      "epoch 83, accurracy on training dataset0.9872833333333333, accuracy on test dataset0.9678， loss on training dataset0.046785734593868256, loss on test dataset0.15174368023872375  \n",
      "epoch 84, accurracy on training dataset0.9874666666666667, accuracy on test dataset0.968， loss on training dataset0.046466074883937836, loss on test dataset0.1524181216955185  \n",
      "epoch 85, accurracy on training dataset0.9874333333333334, accuracy on test dataset0.967， loss on training dataset0.04641801863908768, loss on test dataset0.15375110507011414  \n",
      "epoch 86, accurracy on training dataset0.9875666666666667, accuracy on test dataset0.9672， loss on training dataset0.04596589505672455, loss on test dataset0.15448395907878876  \n",
      "epoch 87, accurracy on training dataset0.9874833333333334, accuracy on test dataset0.967， loss on training dataset0.04560994729399681, loss on test dataset0.1554611325263977  \n",
      "epoch 88, accurracy on training dataset0.9877166666666667, accuracy on test dataset0.9667， loss on training dataset0.045381948351860046, loss on test dataset0.15645654499530792  \n",
      "epoch 89, accurracy on training dataset0.9878, accuracy on test dataset0.9669， loss on training dataset0.045164577662944794, loss on test dataset0.15748624503612518  \n",
      "epoch 90, accurracy on training dataset0.9879333333333333, accuracy on test dataset0.9666， loss on training dataset0.04485616087913513, loss on test dataset0.15854668617248535  \n",
      "epoch 91, accurracy on training dataset0.9879, accuracy on test dataset0.967， loss on training dataset0.044689614325761795, loss on test dataset0.159598246216774  \n",
      "epoch 92, accurracy on training dataset0.9878666666666667, accuracy on test dataset0.9676， loss on training dataset0.04455491900444031, loss on test dataset0.16076292097568512  \n",
      "epoch 93, accurracy on training dataset0.98805, accuracy on test dataset0.9674， loss on training dataset0.044347986578941345, loss on test dataset0.1621742844581604  \n",
      "epoch 94, accurracy on training dataset0.9882166666666666, accuracy on test dataset0.9674， loss on training dataset0.04410004988312721, loss on test dataset0.16357293725013733  \n",
      "epoch 95, accurracy on training dataset0.9881666666666666, accuracy on test dataset0.9675， loss on training dataset0.04393324628472328, loss on test dataset0.16475600004196167  \n",
      "epoch 96, accurracy on training dataset0.9883666666666666, accuracy on test dataset0.9671， loss on training dataset0.04366593062877655, loss on test dataset0.16593891382217407  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97, accurracy on training dataset0.9883, accuracy on test dataset0.9669， loss on training dataset0.04337764158844948, loss on test dataset0.16707034409046173  \n",
      "epoch 98, accurracy on training dataset0.9885333333333334, accuracy on test dataset0.967， loss on training dataset0.043065741658210754, loss on test dataset0.16833870112895966  \n",
      "epoch 99, accurracy on training dataset0.9885166666666667, accuracy on test dataset0.967， loss on training dataset0.04271789640188217, loss on test dataset0.16922391951084137  \n",
      "epoch 100, accurracy on training dataset0.9886833333333334, accuracy on test dataset0.9671， loss on training dataset0.0423758439719677, loss on test dataset0.17008425295352936  \n",
      "epoch 101, accurracy on training dataset0.9889166666666667, accuracy on test dataset0.9669， loss on training dataset0.042089398950338364, loss on test dataset0.17119954526424408  \n",
      "epoch 102, accurracy on training dataset0.98895, accuracy on test dataset0.9665， loss on training dataset0.041892971843481064, loss on test dataset0.17227676510810852  \n",
      "epoch 103, accurracy on training dataset0.9889833333333333, accuracy on test dataset0.9661， loss on training dataset0.041617605835199356, loss on test dataset0.17326781153678894  \n",
      "epoch 104, accurracy on training dataset0.9893, accuracy on test dataset0.966， loss on training dataset0.041404563933610916, loss on test dataset0.17396290600299835  \n",
      "epoch 105, accurracy on training dataset0.9892666666666666, accuracy on test dataset0.9662， loss on training dataset0.041193168610334396, loss on test dataset0.1751866489648819  \n",
      "epoch 106, accurracy on training dataset0.9893833333333333, accuracy on test dataset0.9663， loss on training dataset0.04112080857157707, loss on test dataset0.17637336254119873  \n",
      "epoch 107, accurracy on training dataset0.9892833333333333, accuracy on test dataset0.9658， loss on training dataset0.04081451892852783, loss on test dataset0.17743460834026337  \n",
      "epoch 108, accurracy on training dataset0.9892, accuracy on test dataset0.9656， loss on training dataset0.040697574615478516, loss on test dataset0.17814579606056213  \n",
      "epoch 109, accurracy on training dataset0.9892666666666666, accuracy on test dataset0.9651， loss on training dataset0.040386512875556946, loss on test dataset0.17910921573638916  \n",
      "epoch 110, accurracy on training dataset0.98955, accuracy on test dataset0.9653， loss on training dataset0.040117595344781876, loss on test dataset0.1796858012676239  \n",
      "epoch 111, accurracy on training dataset0.98965, accuracy on test dataset0.965， loss on training dataset0.03987390547990799, loss on test dataset0.1804506927728653  \n",
      "epoch 112, accurracy on training dataset0.9897, accuracy on test dataset0.965， loss on training dataset0.03969592973589897, loss on test dataset0.18086755275726318  \n",
      "epoch 113, accurracy on training dataset0.98965, accuracy on test dataset0.9655， loss on training dataset0.039400819689035416, loss on test dataset0.18142227828502655  \n",
      "epoch 114, accurracy on training dataset0.9896666666666667, accuracy on test dataset0.9659， loss on training dataset0.039263926446437836, loss on test dataset0.1817086786031723  \n",
      "epoch 115, accurracy on training dataset0.9898, accuracy on test dataset0.9658， loss on training dataset0.03900773823261261, loss on test dataset0.18217141926288605  \n",
      "epoch 116, accurracy on training dataset0.9899, accuracy on test dataset0.9663， loss on training dataset0.03867322579026222, loss on test dataset0.18249312043190002  \n",
      "epoch 117, accurracy on training dataset0.9901333333333333, accuracy on test dataset0.9661， loss on training dataset0.03832842782139778, loss on test dataset0.1829211562871933  \n",
      "Finally epoch 117, accurracy on training dataset0.9901333333333333, accuracy on test dataset0.9661 \n"
     ]
    }
   ],
   "source": [
    "train_accuracy_list, test_accuracy_list,ep ,train_loss_list,test_loss_list= fit_gd_cross_entropy(x_train, y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(train_accuracy_list,test_accuracy_list,ep,plot_name,x_test,y_test):\n",
    "    sns.set()\n",
    "    plt.plot(range(1,ep+1),train_accuracy_list,'-',label ='train accuracy')\n",
    "    plt.plot(range(1,ep+1),test_accuracy_list,'-',label ='test accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.savefig('./'+plot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W2():\n",
    "    d = 784\n",
    "    h0 = 32\n",
    "    h1 = 32\n",
    "    k = 10\n",
    "    alpha1 = (1/d)**0.5\n",
    "    alpha2 = (1/h0)**0.5\n",
    "    alpha3 = (1/h1)**0.5\n",
    "    W_0 = np.random.uniform(-alpha1, alpha1,(h0,d))\n",
    "    W_1 = np.random.uniform(-alpha2, alpha2,(h1,h0))\n",
    "    W_2 = np.random.uniform(-alpha3, alpha3,(k,h1))\n",
    "    b_0 = np.zeros((h0,1))\n",
    "    b_1 = np.zeros((h1,1))\n",
    "    b_2 = np.zeros((k,1))\n",
    "    W_0 = torch.FloatTensor(W_0).requires_grad_(True)\n",
    "    W_1 = torch.FloatTensor(W_1).requires_grad_(True)\n",
    "    W_2 = torch.FloatTensor(W_2).requires_grad_(True)\n",
    "    b_2 = torch.FloatTensor(b_2).requires_grad_(True)\n",
    "    b_0 = torch.FloatTensor(b_0).requires_grad_(True)\n",
    "    b_1 = torch.FloatTensor(b_1).requires_grad_(True)\n",
    "    \n",
    "    return W_0,W_1,W_2,b_0,b_1,b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gd_cross_entropy2(x_train, y_train,x_test,y_test):\n",
    "    W_0,W_1,W_2,b_0,b_1,b_2 = init_W2()\n",
    "    loss_prev = 0\n",
    "    d_loss = 1\n",
    "    loss = 1\n",
    "    accuracy = 0\n",
    "    ep = 0\n",
    "    batchsize = 1000\n",
    "    nbatch = int(x_train.shape[0]/batchsize)\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    traccuracy = 0\n",
    "    taccuracy = 0\n",
    "    while traccuracy <0.99:\n",
    "        ep = ep+1\n",
    "        accuracy_list = []\n",
    "        for i in range(0, nbatch):\n",
    "            x = x_train[i*batchsize:(i+1)*batchsize]\n",
    "            y = y_train[i*batchsize:(i+1)*batchsize]\n",
    "            optim = torch.optim.Adam([W_0,W_1,b_0,b_1], lr=1e-2)\n",
    "            x_1 = torch.matmul(W_0, x.T) + b_0\n",
    "            x_1 = torch.nn.functional.relu(x_1)\n",
    "            x_2 = torch.matmul(W_1, x_1) + b_1\n",
    "            x_2 = torch.nn.functional.relu(x_2)\n",
    "            y_hat = torch.matmul(W_2, x_2) + b_2\n",
    "            #y_hat = torch.nn.functional.relu(y_hat)\n",
    "            y_hat = y_hat.T\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y.long())\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            accuracy = float(torch.sum(torch.argmax(y_hat, axis=1) == y)) / float(y.shape[0])\n",
    "            #accuracy_list.append(accuracy)\n",
    "            optim.step()\n",
    "        xtr_1 = torch.matmul(W_0, x_train.T) + b_0\n",
    "        xtr_1 = torch.nn.functional.relu(xtr_1)\n",
    "        xtr_2 = torch.matmul(W_1, xtr_1) + b_1\n",
    "        xtr_2 = torch.nn.functional.relu(xtr_2)\n",
    "        ytr_hat = torch.matmul(W_2, xtr_2) + b_2\n",
    "        #yt_hat = torch.nn.functional.relu(yt_hat)\n",
    "        ytr_hat = ytr_hat.T\n",
    "        traccuracy = float(torch.sum(torch.argmax(ytr_hat, axis=1) == y_train)) / float(y_train.shape[0])\n",
    "        xt_1 = torch.matmul(W_0, x_test.T) + b_0\n",
    "        xt_1 = torch.nn.functional.relu(xt_1)\n",
    "        xt_2 = torch.matmul(W_1, xt_1) + b_1\n",
    "        xt_2 = torch.nn.functional.relu(xt_2)\n",
    "        yt_hat = torch.matmul(W_2, xt_2) + b_2\n",
    "        #yt_hat = torch.nn.functional.relu(yt_hat)\n",
    "        yt_hat = yt_hat.T\n",
    "        taccuracy = float(torch.sum(torch.argmax(yt_hat, axis=1) == y_test)) / float(y_test.shape[0])\n",
    "        train_accuracy_list.append(traccuracy)\n",
    "        test_accuracy_list.append(taccuracy)\n",
    "        #print(\"epoch {}, accurracy on training dataset{}, accuracy on test dataset{} \".format(ep,traccuracy,taccuracy))    \n",
    "    print(\"Finally epoch {}, accurracy on training dataset{}, accuracy on test dataset{} \".format(ep,traccuracy,taccuracy))        \n",
    "    return(train_accuracy_list, test_accuracy_list,ep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, accurracy on training dataset0.82695, accuracy on test dataset0.8369 \n",
      "epoch 2, accurracy on training dataset0.89, accuracy on test dataset0.8947 \n",
      "epoch 3, accurracy on training dataset0.9152, accuracy on test dataset0.916 \n",
      "epoch 4, accurracy on training dataset0.9252166666666667, accuracy on test dataset0.924 \n",
      "epoch 5, accurracy on training dataset0.9329, accuracy on test dataset0.9288 \n",
      "epoch 6, accurracy on training dataset0.9372333333333334, accuracy on test dataset0.9338 \n",
      "epoch 7, accurracy on training dataset0.9454, accuracy on test dataset0.9412 \n",
      "epoch 8, accurracy on training dataset0.9473333333333334, accuracy on test dataset0.942 \n",
      "epoch 9, accurracy on training dataset0.95145, accuracy on test dataset0.9471 \n",
      "epoch 10, accurracy on training dataset0.95615, accuracy on test dataset0.9506 \n",
      "epoch 11, accurracy on training dataset0.9557166666666667, accuracy on test dataset0.9473 \n",
      "epoch 12, accurracy on training dataset0.9561333333333333, accuracy on test dataset0.9488 \n",
      "epoch 13, accurracy on training dataset0.9558666666666666, accuracy on test dataset0.9468 \n",
      "epoch 14, accurracy on training dataset0.9620166666666666, accuracy on test dataset0.953 \n",
      "epoch 15, accurracy on training dataset0.9613166666666667, accuracy on test dataset0.9532 \n",
      "epoch 16, accurracy on training dataset0.9629833333333333, accuracy on test dataset0.9536 \n",
      "epoch 17, accurracy on training dataset0.9640333333333333, accuracy on test dataset0.9551 \n",
      "epoch 18, accurracy on training dataset0.9661666666666666, accuracy on test dataset0.9557 \n",
      "epoch 19, accurracy on training dataset0.96035, accuracy on test dataset0.9499 \n",
      "epoch 20, accurracy on training dataset0.9663166666666667, accuracy on test dataset0.9552 \n",
      "epoch 21, accurracy on training dataset0.96785, accuracy on test dataset0.9558 \n",
      "epoch 22, accurracy on training dataset0.96755, accuracy on test dataset0.9558 \n",
      "epoch 23, accurracy on training dataset0.9694666666666667, accuracy on test dataset0.9569 \n",
      "epoch 24, accurracy on training dataset0.96885, accuracy on test dataset0.9564 \n",
      "epoch 25, accurracy on training dataset0.9708333333333333, accuracy on test dataset0.9575 \n",
      "epoch 26, accurracy on training dataset0.9690666666666666, accuracy on test dataset0.9537 \n",
      "epoch 27, accurracy on training dataset0.9715833333333334, accuracy on test dataset0.9575 \n",
      "epoch 28, accurracy on training dataset0.9731666666666666, accuracy on test dataset0.9566 \n",
      "epoch 29, accurracy on training dataset0.9728666666666667, accuracy on test dataset0.957 \n",
      "epoch 30, accurracy on training dataset0.9726833333333333, accuracy on test dataset0.9579 \n",
      "epoch 31, accurracy on training dataset0.9744166666666667, accuracy on test dataset0.9592 \n",
      "epoch 32, accurracy on training dataset0.9741833333333333, accuracy on test dataset0.957 \n",
      "epoch 33, accurracy on training dataset0.9729833333333333, accuracy on test dataset0.9568 \n",
      "epoch 34, accurracy on training dataset0.97495, accuracy on test dataset0.9601 \n",
      "epoch 35, accurracy on training dataset0.9747166666666667, accuracy on test dataset0.9578 \n",
      "epoch 36, accurracy on training dataset0.97465, accuracy on test dataset0.9581 \n",
      "epoch 37, accurracy on training dataset0.974, accuracy on test dataset0.9592 \n",
      "epoch 38, accurracy on training dataset0.975, accuracy on test dataset0.9591 \n",
      "epoch 39, accurracy on training dataset0.9755333333333334, accuracy on test dataset0.9578 \n",
      "epoch 40, accurracy on training dataset0.97495, accuracy on test dataset0.9591 \n",
      "epoch 41, accurracy on training dataset0.97435, accuracy on test dataset0.9542 \n",
      "epoch 42, accurracy on training dataset0.9748666666666667, accuracy on test dataset0.96 \n",
      "epoch 43, accurracy on training dataset0.97545, accuracy on test dataset0.9606 \n",
      "epoch 44, accurracy on training dataset0.9761166666666666, accuracy on test dataset0.9586 \n",
      "epoch 45, accurracy on training dataset0.9754666666666667, accuracy on test dataset0.9559 \n",
      "epoch 46, accurracy on training dataset0.9761833333333333, accuracy on test dataset0.9571 \n",
      "epoch 47, accurracy on training dataset0.97635, accuracy on test dataset0.9579 \n",
      "epoch 48, accurracy on training dataset0.9746333333333334, accuracy on test dataset0.9547 \n",
      "epoch 49, accurracy on training dataset0.97625, accuracy on test dataset0.9567 \n",
      "epoch 50, accurracy on training dataset0.97555, accuracy on test dataset0.9571 \n",
      "epoch 51, accurracy on training dataset0.9757166666666667, accuracy on test dataset0.9551 \n",
      "epoch 52, accurracy on training dataset0.9748666666666667, accuracy on test dataset0.9563 \n",
      "epoch 53, accurracy on training dataset0.9749666666666666, accuracy on test dataset0.9552 \n",
      "epoch 54, accurracy on training dataset0.97745, accuracy on test dataset0.9568 \n",
      "epoch 55, accurracy on training dataset0.9765666666666667, accuracy on test dataset0.9573 \n",
      "epoch 56, accurracy on training dataset0.97605, accuracy on test dataset0.9573 \n",
      "epoch 57, accurracy on training dataset0.9768833333333333, accuracy on test dataset0.9562 \n",
      "epoch 58, accurracy on training dataset0.9772333333333333, accuracy on test dataset0.9565 \n",
      "epoch 59, accurracy on training dataset0.97785, accuracy on test dataset0.9563 \n",
      "epoch 60, accurracy on training dataset0.9764833333333334, accuracy on test dataset0.9569 \n",
      "epoch 61, accurracy on training dataset0.9789, accuracy on test dataset0.9557 \n",
      "epoch 62, accurracy on training dataset0.9752666666666666, accuracy on test dataset0.9537 \n",
      "epoch 63, accurracy on training dataset0.9801, accuracy on test dataset0.9573 \n",
      "epoch 64, accurracy on training dataset0.9746666666666667, accuracy on test dataset0.955 \n",
      "epoch 65, accurracy on training dataset0.9776666666666667, accuracy on test dataset0.9556 \n",
      "epoch 66, accurracy on training dataset0.9775833333333334, accuracy on test dataset0.9546 \n",
      "epoch 67, accurracy on training dataset0.9781166666666666, accuracy on test dataset0.9554 \n",
      "epoch 68, accurracy on training dataset0.9773, accuracy on test dataset0.9546 \n",
      "epoch 69, accurracy on training dataset0.97645, accuracy on test dataset0.9534 \n",
      "epoch 70, accurracy on training dataset0.97505, accuracy on test dataset0.954 \n",
      "epoch 71, accurracy on training dataset0.9782666666666666, accuracy on test dataset0.955 \n",
      "epoch 72, accurracy on training dataset0.9767166666666667, accuracy on test dataset0.9544 \n",
      "epoch 73, accurracy on training dataset0.9775666666666667, accuracy on test dataset0.9562 \n",
      "epoch 74, accurracy on training dataset0.9766666666666667, accuracy on test dataset0.9547 \n",
      "epoch 75, accurracy on training dataset0.9788666666666667, accuracy on test dataset0.9567 \n",
      "epoch 76, accurracy on training dataset0.9777333333333333, accuracy on test dataset0.9556 \n",
      "epoch 77, accurracy on training dataset0.9801, accuracy on test dataset0.9562 \n",
      "epoch 78, accurracy on training dataset0.9806333333333334, accuracy on test dataset0.9547 \n",
      "epoch 79, accurracy on training dataset0.9797, accuracy on test dataset0.956 \n",
      "epoch 80, accurracy on training dataset0.9784666666666667, accuracy on test dataset0.9546 \n",
      "epoch 81, accurracy on training dataset0.9779, accuracy on test dataset0.955 \n",
      "epoch 82, accurracy on training dataset0.98, accuracy on test dataset0.9546 \n",
      "epoch 83, accurracy on training dataset0.9790666666666666, accuracy on test dataset0.9539 \n",
      "epoch 84, accurracy on training dataset0.9806333333333334, accuracy on test dataset0.9554 \n",
      "epoch 85, accurracy on training dataset0.9814666666666667, accuracy on test dataset0.9556 \n",
      "epoch 86, accurracy on training dataset0.9804666666666667, accuracy on test dataset0.9545 \n",
      "epoch 87, accurracy on training dataset0.9788333333333333, accuracy on test dataset0.9547 \n",
      "epoch 88, accurracy on training dataset0.9789833333333333, accuracy on test dataset0.9562 \n",
      "epoch 89, accurracy on training dataset0.9792833333333333, accuracy on test dataset0.9542 \n",
      "epoch 90, accurracy on training dataset0.98065, accuracy on test dataset0.955 \n",
      "epoch 91, accurracy on training dataset0.9795666666666667, accuracy on test dataset0.9549 \n",
      "epoch 92, accurracy on training dataset0.9800833333333333, accuracy on test dataset0.9548 \n",
      "epoch 93, accurracy on training dataset0.98125, accuracy on test dataset0.9547 \n",
      "epoch 94, accurracy on training dataset0.98095, accuracy on test dataset0.9563 \n",
      "epoch 95, accurracy on training dataset0.9807833333333333, accuracy on test dataset0.9573 \n",
      "epoch 96, accurracy on training dataset0.9829333333333333, accuracy on test dataset0.958 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97, accurracy on training dataset0.9819666666666667, accuracy on test dataset0.9554 \n",
      "epoch 98, accurracy on training dataset0.9824, accuracy on test dataset0.9565 \n",
      "epoch 99, accurracy on training dataset0.9828166666666667, accuracy on test dataset0.9559 \n",
      "epoch 100, accurracy on training dataset0.9807833333333333, accuracy on test dataset0.955 \n",
      "epoch 101, accurracy on training dataset0.9822666666666666, accuracy on test dataset0.9571 \n",
      "epoch 102, accurracy on training dataset0.98225, accuracy on test dataset0.9545 \n",
      "epoch 103, accurracy on training dataset0.9812166666666666, accuracy on test dataset0.955 \n",
      "epoch 104, accurracy on training dataset0.9824666666666667, accuracy on test dataset0.9558 \n",
      "epoch 105, accurracy on training dataset0.9808166666666667, accuracy on test dataset0.9546 \n",
      "epoch 106, accurracy on training dataset0.9816666666666667, accuracy on test dataset0.9561 \n",
      "epoch 107, accurracy on training dataset0.9801, accuracy on test dataset0.953 \n",
      "epoch 108, accurracy on training dataset0.97915, accuracy on test dataset0.9541 \n",
      "epoch 109, accurracy on training dataset0.9804333333333334, accuracy on test dataset0.9542 \n",
      "epoch 110, accurracy on training dataset0.9798166666666667, accuracy on test dataset0.9533 \n",
      "epoch 111, accurracy on training dataset0.9816, accuracy on test dataset0.953 \n",
      "epoch 112, accurracy on training dataset0.9813833333333334, accuracy on test dataset0.9544 \n",
      "epoch 113, accurracy on training dataset0.9801333333333333, accuracy on test dataset0.9546 \n",
      "epoch 114, accurracy on training dataset0.9808166666666667, accuracy on test dataset0.955 \n",
      "epoch 115, accurracy on training dataset0.9812166666666666, accuracy on test dataset0.9556 \n",
      "epoch 116, accurracy on training dataset0.9814666666666667, accuracy on test dataset0.955 \n",
      "epoch 117, accurracy on training dataset0.9792666666666666, accuracy on test dataset0.9527 \n",
      "epoch 118, accurracy on training dataset0.9799, accuracy on test dataset0.954 \n",
      "epoch 119, accurracy on training dataset0.9795, accuracy on test dataset0.9537 \n",
      "epoch 120, accurracy on training dataset0.9789, accuracy on test dataset0.9512 \n",
      "epoch 121, accurracy on training dataset0.9826333333333334, accuracy on test dataset0.955 \n",
      "epoch 122, accurracy on training dataset0.98035, accuracy on test dataset0.954 \n",
      "epoch 123, accurracy on training dataset0.98055, accuracy on test dataset0.9544 \n",
      "epoch 124, accurracy on training dataset0.9805166666666667, accuracy on test dataset0.9547 \n",
      "epoch 125, accurracy on training dataset0.9818333333333333, accuracy on test dataset0.9553 \n",
      "epoch 126, accurracy on training dataset0.9837833333333333, accuracy on test dataset0.9564 \n",
      "epoch 127, accurracy on training dataset0.9795166666666667, accuracy on test dataset0.9531 \n",
      "epoch 128, accurracy on training dataset0.9818666666666667, accuracy on test dataset0.9554 \n",
      "epoch 129, accurracy on training dataset0.9802166666666666, accuracy on test dataset0.9545 \n",
      "epoch 130, accurracy on training dataset0.97855, accuracy on test dataset0.9528 \n",
      "epoch 131, accurracy on training dataset0.9831166666666666, accuracy on test dataset0.9539 \n",
      "epoch 132, accurracy on training dataset0.9771, accuracy on test dataset0.9503 \n",
      "epoch 133, accurracy on training dataset0.9804833333333334, accuracy on test dataset0.9535 \n",
      "epoch 134, accurracy on training dataset0.9794333333333334, accuracy on test dataset0.9517 \n",
      "epoch 135, accurracy on training dataset0.97895, accuracy on test dataset0.9507 \n",
      "epoch 136, accurracy on training dataset0.9787666666666667, accuracy on test dataset0.9534 \n",
      "epoch 137, accurracy on training dataset0.9805333333333334, accuracy on test dataset0.9536 \n",
      "epoch 138, accurracy on training dataset0.9795, accuracy on test dataset0.9532 \n",
      "epoch 139, accurracy on training dataset0.9815666666666667, accuracy on test dataset0.9542 \n",
      "epoch 140, accurracy on training dataset0.9805666666666667, accuracy on test dataset0.9535 \n",
      "epoch 141, accurracy on training dataset0.9834, accuracy on test dataset0.954 \n",
      "epoch 142, accurracy on training dataset0.98345, accuracy on test dataset0.9542 \n",
      "epoch 143, accurracy on training dataset0.9834666666666667, accuracy on test dataset0.9526 \n",
      "epoch 144, accurracy on training dataset0.98415, accuracy on test dataset0.9551 \n",
      "epoch 145, accurracy on training dataset0.9824666666666667, accuracy on test dataset0.9536 \n",
      "epoch 146, accurracy on training dataset0.9811666666666666, accuracy on test dataset0.9534 \n",
      "epoch 147, accurracy on training dataset0.98145, accuracy on test dataset0.9533 \n",
      "epoch 148, accurracy on training dataset0.9811333333333333, accuracy on test dataset0.9532 \n",
      "epoch 149, accurracy on training dataset0.9805333333333334, accuracy on test dataset0.9537 \n",
      "epoch 150, accurracy on training dataset0.9806166666666667, accuracy on test dataset0.953 \n",
      "epoch 151, accurracy on training dataset0.9832833333333333, accuracy on test dataset0.9535 \n",
      "epoch 152, accurracy on training dataset0.9844166666666667, accuracy on test dataset0.9534 \n",
      "epoch 153, accurracy on training dataset0.9799833333333333, accuracy on test dataset0.9517 \n",
      "epoch 154, accurracy on training dataset0.9827166666666667, accuracy on test dataset0.9525 \n",
      "epoch 155, accurracy on training dataset0.9806833333333334, accuracy on test dataset0.9542 \n",
      "epoch 156, accurracy on training dataset0.9792333333333333, accuracy on test dataset0.9508 \n",
      "epoch 157, accurracy on training dataset0.9835833333333334, accuracy on test dataset0.9541 \n",
      "epoch 158, accurracy on training dataset0.98125, accuracy on test dataset0.9522 \n",
      "epoch 159, accurracy on training dataset0.9826833333333334, accuracy on test dataset0.9526 \n",
      "epoch 160, accurracy on training dataset0.9839333333333333, accuracy on test dataset0.9538 \n",
      "epoch 161, accurracy on training dataset0.9857166666666667, accuracy on test dataset0.9524 \n",
      "epoch 162, accurracy on training dataset0.9842833333333333, accuracy on test dataset0.953 \n",
      "epoch 163, accurracy on training dataset0.9845333333333334, accuracy on test dataset0.953 \n",
      "epoch 164, accurracy on training dataset0.98205, accuracy on test dataset0.9535 \n",
      "epoch 165, accurracy on training dataset0.9837, accuracy on test dataset0.9539 \n",
      "epoch 166, accurracy on training dataset0.9851833333333333, accuracy on test dataset0.9525 \n",
      "epoch 167, accurracy on training dataset0.9852833333333333, accuracy on test dataset0.9523 \n",
      "epoch 168, accurracy on training dataset0.98695, accuracy on test dataset0.9539 \n",
      "epoch 169, accurracy on training dataset0.9843166666666666, accuracy on test dataset0.9525 \n",
      "epoch 170, accurracy on training dataset0.9826333333333334, accuracy on test dataset0.9517 \n",
      "epoch 171, accurracy on training dataset0.983, accuracy on test dataset0.9518 \n",
      "epoch 172, accurracy on training dataset0.9847666666666667, accuracy on test dataset0.9528 \n",
      "epoch 173, accurracy on training dataset0.9847666666666667, accuracy on test dataset0.9541 \n",
      "epoch 174, accurracy on training dataset0.9843333333333333, accuracy on test dataset0.9538 \n",
      "epoch 175, accurracy on training dataset0.9843666666666666, accuracy on test dataset0.953 \n",
      "epoch 176, accurracy on training dataset0.9854333333333334, accuracy on test dataset0.9528 \n",
      "epoch 177, accurracy on training dataset0.9830833333333333, accuracy on test dataset0.9521 \n",
      "epoch 178, accurracy on training dataset0.9855833333333334, accuracy on test dataset0.9529 \n",
      "epoch 179, accurracy on training dataset0.98505, accuracy on test dataset0.9511 \n",
      "epoch 180, accurracy on training dataset0.9847666666666667, accuracy on test dataset0.9511 \n",
      "epoch 181, accurracy on training dataset0.9856666666666667, accuracy on test dataset0.9514 \n",
      "epoch 182, accurracy on training dataset0.9858833333333333, accuracy on test dataset0.9518 \n",
      "epoch 183, accurracy on training dataset0.9849833333333333, accuracy on test dataset0.9514 \n",
      "epoch 184, accurracy on training dataset0.9870333333333333, accuracy on test dataset0.951 \n",
      "epoch 185, accurracy on training dataset0.9867166666666667, accuracy on test dataset0.9517 \n",
      "epoch 186, accurracy on training dataset0.9847333333333333, accuracy on test dataset0.9509 \n",
      "epoch 187, accurracy on training dataset0.9837666666666667, accuracy on test dataset0.9504 \n",
      "epoch 188, accurracy on training dataset0.9847666666666667, accuracy on test dataset0.9514 \n",
      "epoch 189, accurracy on training dataset0.9861, accuracy on test dataset0.9506 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 190, accurracy on training dataset0.9851833333333333, accuracy on test dataset0.9501 \n",
      "epoch 191, accurracy on training dataset0.9856333333333334, accuracy on test dataset0.9516 \n",
      "epoch 192, accurracy on training dataset0.9869666666666667, accuracy on test dataset0.951 \n",
      "epoch 193, accurracy on training dataset0.9872333333333333, accuracy on test dataset0.9515 \n",
      "epoch 194, accurracy on training dataset0.9861, accuracy on test dataset0.9499 \n",
      "epoch 195, accurracy on training dataset0.9863333333333333, accuracy on test dataset0.9516 \n",
      "epoch 196, accurracy on training dataset0.9859, accuracy on test dataset0.9518 \n",
      "epoch 197, accurracy on training dataset0.9847333333333333, accuracy on test dataset0.9529 \n",
      "epoch 198, accurracy on training dataset0.98565, accuracy on test dataset0.9521 \n",
      "epoch 199, accurracy on training dataset0.9867833333333333, accuracy on test dataset0.9522 \n",
      "epoch 200, accurracy on training dataset0.9879333333333333, accuracy on test dataset0.9519 \n",
      "epoch 201, accurracy on training dataset0.9868666666666667, accuracy on test dataset0.9527 \n",
      "epoch 202, accurracy on training dataset0.987, accuracy on test dataset0.9529 \n",
      "epoch 203, accurracy on training dataset0.9870166666666667, accuracy on test dataset0.9513 \n",
      "epoch 204, accurracy on training dataset0.9852166666666666, accuracy on test dataset0.9505 \n",
      "epoch 205, accurracy on training dataset0.9859333333333333, accuracy on test dataset0.951 \n",
      "epoch 206, accurracy on training dataset0.9876666666666667, accuracy on test dataset0.9525 \n",
      "epoch 207, accurracy on training dataset0.9879666666666667, accuracy on test dataset0.9528 \n",
      "epoch 208, accurracy on training dataset0.9862333333333333, accuracy on test dataset0.953 \n",
      "epoch 209, accurracy on training dataset0.9871, accuracy on test dataset0.9526 \n",
      "epoch 210, accurracy on training dataset0.9871166666666666, accuracy on test dataset0.9525 \n",
      "epoch 211, accurracy on training dataset0.9851833333333333, accuracy on test dataset0.9521 \n",
      "epoch 212, accurracy on training dataset0.9846666666666667, accuracy on test dataset0.9508 \n",
      "epoch 213, accurracy on training dataset0.9854833333333334, accuracy on test dataset0.9529 \n",
      "epoch 214, accurracy on training dataset0.9875333333333334, accuracy on test dataset0.9516 \n",
      "epoch 215, accurracy on training dataset0.98675, accuracy on test dataset0.9509 \n",
      "epoch 216, accurracy on training dataset0.9873, accuracy on test dataset0.9524 \n",
      "epoch 217, accurracy on training dataset0.9869833333333333, accuracy on test dataset0.9528 \n",
      "epoch 218, accurracy on training dataset0.9845333333333334, accuracy on test dataset0.9512 \n",
      "epoch 219, accurracy on training dataset0.9864, accuracy on test dataset0.9511 \n",
      "epoch 220, accurracy on training dataset0.9852333333333333, accuracy on test dataset0.9507 \n",
      "epoch 221, accurracy on training dataset0.9861833333333333, accuracy on test dataset0.9517 \n",
      "epoch 222, accurracy on training dataset0.98605, accuracy on test dataset0.9516 \n",
      "epoch 223, accurracy on training dataset0.9841, accuracy on test dataset0.9514 \n",
      "epoch 224, accurracy on training dataset0.9866833333333334, accuracy on test dataset0.9527 \n",
      "epoch 225, accurracy on training dataset0.9851333333333333, accuracy on test dataset0.9516 \n",
      "epoch 226, accurracy on training dataset0.9863, accuracy on test dataset0.9515 \n",
      "epoch 227, accurracy on training dataset0.9857333333333334, accuracy on test dataset0.9515 \n",
      "epoch 228, accurracy on training dataset0.9845666666666667, accuracy on test dataset0.9509 \n",
      "epoch 229, accurracy on training dataset0.9852166666666666, accuracy on test dataset0.9508 \n",
      "epoch 230, accurracy on training dataset0.9858166666666667, accuracy on test dataset0.9495 \n",
      "epoch 231, accurracy on training dataset0.9859333333333333, accuracy on test dataset0.9508 \n",
      "epoch 232, accurracy on training dataset0.98465, accuracy on test dataset0.9507 \n",
      "epoch 233, accurracy on training dataset0.9856666666666667, accuracy on test dataset0.9507 \n",
      "epoch 234, accurracy on training dataset0.9852, accuracy on test dataset0.9498 \n",
      "epoch 235, accurracy on training dataset0.9853666666666666, accuracy on test dataset0.9522 \n",
      "epoch 236, accurracy on training dataset0.9841666666666666, accuracy on test dataset0.9513 \n",
      "epoch 237, accurracy on training dataset0.9836333333333334, accuracy on test dataset0.9504 \n",
      "epoch 238, accurracy on training dataset0.9841, accuracy on test dataset0.9511 \n",
      "epoch 239, accurracy on training dataset0.9838, accuracy on test dataset0.9516 \n",
      "epoch 240, accurracy on training dataset0.9861666666666666, accuracy on test dataset0.9528 \n",
      "epoch 241, accurracy on training dataset0.9867166666666667, accuracy on test dataset0.9512 \n",
      "epoch 242, accurracy on training dataset0.9842, accuracy on test dataset0.9522 \n",
      "epoch 243, accurracy on training dataset0.9863833333333333, accuracy on test dataset0.9526 \n",
      "epoch 244, accurracy on training dataset0.9878666666666667, accuracy on test dataset0.9525 \n",
      "epoch 245, accurracy on training dataset0.98715, accuracy on test dataset0.9538 \n",
      "epoch 246, accurracy on training dataset0.9873, accuracy on test dataset0.9532 \n",
      "epoch 247, accurracy on training dataset0.98635, accuracy on test dataset0.9544 \n",
      "epoch 248, accurracy on training dataset0.9853166666666666, accuracy on test dataset0.9543 \n",
      "epoch 249, accurracy on training dataset0.9846166666666667, accuracy on test dataset0.9539 \n",
      "epoch 250, accurracy on training dataset0.9863166666666666, accuracy on test dataset0.954 \n",
      "epoch 251, accurracy on training dataset0.9842833333333333, accuracy on test dataset0.954 \n",
      "epoch 252, accurracy on training dataset0.9863666666666666, accuracy on test dataset0.9543 \n",
      "epoch 253, accurracy on training dataset0.9858666666666667, accuracy on test dataset0.9548 \n",
      "epoch 254, accurracy on training dataset0.9876833333333334, accuracy on test dataset0.955 \n",
      "epoch 255, accurracy on training dataset0.9860333333333333, accuracy on test dataset0.9563 \n",
      "epoch 256, accurracy on training dataset0.9868333333333333, accuracy on test dataset0.9541 \n",
      "epoch 257, accurracy on training dataset0.9872333333333333, accuracy on test dataset0.9557 \n",
      "epoch 258, accurracy on training dataset0.9875833333333334, accuracy on test dataset0.9533 \n",
      "epoch 259, accurracy on training dataset0.98625, accuracy on test dataset0.9535 \n",
      "epoch 260, accurracy on training dataset0.9881333333333333, accuracy on test dataset0.9548 \n",
      "epoch 261, accurracy on training dataset0.9865, accuracy on test dataset0.955 \n",
      "epoch 262, accurracy on training dataset0.9876166666666667, accuracy on test dataset0.9538 \n",
      "epoch 263, accurracy on training dataset0.9891333333333333, accuracy on test dataset0.954 \n",
      "epoch 264, accurracy on training dataset0.9866666666666667, accuracy on test dataset0.9558 \n",
      "epoch 265, accurracy on training dataset0.9890666666666666, accuracy on test dataset0.9561 \n",
      "epoch 266, accurracy on training dataset0.9868166666666667, accuracy on test dataset0.9552 \n",
      "epoch 267, accurracy on training dataset0.9865833333333334, accuracy on test dataset0.9542 \n",
      "epoch 268, accurracy on training dataset0.9872, accuracy on test dataset0.9555 \n",
      "epoch 269, accurracy on training dataset0.9857166666666667, accuracy on test dataset0.9552 \n",
      "epoch 270, accurracy on training dataset0.98615, accuracy on test dataset0.9544 \n",
      "epoch 271, accurracy on training dataset0.9870166666666667, accuracy on test dataset0.9547 \n",
      "epoch 272, accurracy on training dataset0.9863666666666666, accuracy on test dataset0.9541 \n",
      "epoch 273, accurracy on training dataset0.9864666666666667, accuracy on test dataset0.9551 \n",
      "epoch 274, accurracy on training dataset0.98505, accuracy on test dataset0.9547 \n",
      "epoch 275, accurracy on training dataset0.9859833333333333, accuracy on test dataset0.9565 \n",
      "epoch 276, accurracy on training dataset0.9841333333333333, accuracy on test dataset0.9536 \n",
      "epoch 277, accurracy on training dataset0.9854666666666667, accuracy on test dataset0.9551 \n",
      "epoch 278, accurracy on training dataset0.9865, accuracy on test dataset0.9548 \n",
      "epoch 279, accurracy on training dataset0.985, accuracy on test dataset0.9544 \n",
      "epoch 280, accurracy on training dataset0.9865, accuracy on test dataset0.9558 \n",
      "epoch 281, accurracy on training dataset0.9847, accuracy on test dataset0.9543 \n",
      "epoch 282, accurracy on training dataset0.9861666666666666, accuracy on test dataset0.9551 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 283, accurracy on training dataset0.9870833333333333, accuracy on test dataset0.9547 \n",
      "epoch 284, accurracy on training dataset0.9879833333333333, accuracy on test dataset0.9565 \n",
      "epoch 285, accurracy on training dataset0.9867, accuracy on test dataset0.9547 \n",
      "epoch 286, accurracy on training dataset0.9851333333333333, accuracy on test dataset0.9543 \n",
      "epoch 287, accurracy on training dataset0.9878, accuracy on test dataset0.9546 \n",
      "epoch 288, accurracy on training dataset0.9875833333333334, accuracy on test dataset0.9546 \n",
      "epoch 289, accurracy on training dataset0.9856166666666667, accuracy on test dataset0.9543 \n",
      "epoch 290, accurracy on training dataset0.9858833333333333, accuracy on test dataset0.9551 \n",
      "epoch 291, accurracy on training dataset0.9841666666666666, accuracy on test dataset0.9543 \n",
      "epoch 292, accurracy on training dataset0.9862666666666666, accuracy on test dataset0.9557 \n",
      "epoch 293, accurracy on training dataset0.9863833333333333, accuracy on test dataset0.9554 \n",
      "epoch 294, accurracy on training dataset0.9862833333333333, accuracy on test dataset0.9549 \n",
      "epoch 295, accurracy on training dataset0.9858666666666667, accuracy on test dataset0.9557 \n",
      "epoch 296, accurracy on training dataset0.9859333333333333, accuracy on test dataset0.9559 \n",
      "epoch 297, accurracy on training dataset0.98645, accuracy on test dataset0.9549 \n",
      "epoch 298, accurracy on training dataset0.9868666666666667, accuracy on test dataset0.9551 \n",
      "epoch 299, accurracy on training dataset0.9850666666666666, accuracy on test dataset0.9552 \n",
      "epoch 300, accurracy on training dataset0.9858833333333333, accuracy on test dataset0.9544 \n",
      "epoch 301, accurracy on training dataset0.9836166666666667, accuracy on test dataset0.9541 \n",
      "epoch 302, accurracy on training dataset0.9847333333333333, accuracy on test dataset0.9548 \n",
      "epoch 303, accurracy on training dataset0.9857833333333333, accuracy on test dataset0.9558 \n",
      "epoch 304, accurracy on training dataset0.9835, accuracy on test dataset0.9542 \n",
      "epoch 305, accurracy on training dataset0.9845166666666667, accuracy on test dataset0.9531 \n",
      "epoch 306, accurracy on training dataset0.9871666666666666, accuracy on test dataset0.9554 \n",
      "epoch 307, accurracy on training dataset0.9841166666666666, accuracy on test dataset0.9544 \n",
      "epoch 308, accurracy on training dataset0.9848833333333333, accuracy on test dataset0.9551 \n",
      "epoch 309, accurracy on training dataset0.98635, accuracy on test dataset0.9552 \n",
      "epoch 310, accurracy on training dataset0.9852166666666666, accuracy on test dataset0.9546 \n",
      "epoch 311, accurracy on training dataset0.9880166666666667, accuracy on test dataset0.9561 \n",
      "epoch 312, accurracy on training dataset0.9875333333333334, accuracy on test dataset0.957 \n",
      "epoch 313, accurracy on training dataset0.9852833333333333, accuracy on test dataset0.9539 \n",
      "epoch 314, accurracy on training dataset0.9833166666666666, accuracy on test dataset0.9533 \n",
      "epoch 315, accurracy on training dataset0.9836666666666667, accuracy on test dataset0.9544 \n",
      "epoch 316, accurracy on training dataset0.9855666666666667, accuracy on test dataset0.9531 \n",
      "epoch 317, accurracy on training dataset0.98805, accuracy on test dataset0.9553 \n",
      "epoch 318, accurracy on training dataset0.9848833333333333, accuracy on test dataset0.9534 \n",
      "epoch 319, accurracy on training dataset0.9836333333333334, accuracy on test dataset0.9523 \n",
      "epoch 320, accurracy on training dataset0.9855166666666667, accuracy on test dataset0.9541 \n",
      "epoch 321, accurracy on training dataset0.9840166666666667, accuracy on test dataset0.9527 \n",
      "epoch 322, accurracy on training dataset0.98245, accuracy on test dataset0.9514 \n",
      "epoch 323, accurracy on training dataset0.98605, accuracy on test dataset0.9536 \n",
      "epoch 324, accurracy on training dataset0.9840666666666666, accuracy on test dataset0.9522 \n",
      "epoch 325, accurracy on training dataset0.9847833333333333, accuracy on test dataset0.9525 \n",
      "epoch 326, accurracy on training dataset0.9834333333333334, accuracy on test dataset0.9527 \n",
      "epoch 327, accurracy on training dataset0.9851666666666666, accuracy on test dataset0.9529 \n",
      "epoch 328, accurracy on training dataset0.9871833333333333, accuracy on test dataset0.9539 \n",
      "epoch 329, accurracy on training dataset0.98665, accuracy on test dataset0.9533 \n",
      "epoch 330, accurracy on training dataset0.9849333333333333, accuracy on test dataset0.9525 \n",
      "epoch 331, accurracy on training dataset0.9858666666666667, accuracy on test dataset0.9534 \n",
      "epoch 332, accurracy on training dataset0.9866166666666667, accuracy on test dataset0.9527 \n",
      "epoch 333, accurracy on training dataset0.98745, accuracy on test dataset0.9531 \n",
      "epoch 334, accurracy on training dataset0.987, accuracy on test dataset0.9533 \n",
      "epoch 335, accurracy on training dataset0.9857, accuracy on test dataset0.9522 \n",
      "epoch 336, accurracy on training dataset0.98505, accuracy on test dataset0.9519 \n",
      "epoch 337, accurracy on training dataset0.98795, accuracy on test dataset0.9548 \n",
      "epoch 338, accurracy on training dataset0.98895, accuracy on test dataset0.9541 \n",
      "epoch 339, accurracy on training dataset0.9863333333333333, accuracy on test dataset0.9523 \n",
      "epoch 340, accurracy on training dataset0.9881833333333333, accuracy on test dataset0.9529 \n",
      "epoch 341, accurracy on training dataset0.9883166666666666, accuracy on test dataset0.954 \n",
      "epoch 342, accurracy on training dataset0.9858833333333333, accuracy on test dataset0.9502 \n",
      "epoch 343, accurracy on training dataset0.9870666666666666, accuracy on test dataset0.9511 \n",
      "epoch 344, accurracy on training dataset0.9869666666666667, accuracy on test dataset0.9524 \n",
      "epoch 345, accurracy on training dataset0.9859166666666667, accuracy on test dataset0.9527 \n",
      "epoch 346, accurracy on training dataset0.9861666666666666, accuracy on test dataset0.9521 \n",
      "epoch 347, accurracy on training dataset0.9860166666666667, accuracy on test dataset0.9529 \n",
      "epoch 348, accurracy on training dataset0.98445, accuracy on test dataset0.9531 \n",
      "epoch 349, accurracy on training dataset0.9854833333333334, accuracy on test dataset0.952 \n",
      "epoch 350, accurracy on training dataset0.9864333333333334, accuracy on test dataset0.9518 \n",
      "epoch 351, accurracy on training dataset0.98585, accuracy on test dataset0.9535 \n",
      "epoch 352, accurracy on training dataset0.9857833333333333, accuracy on test dataset0.9524 \n",
      "epoch 353, accurracy on training dataset0.9860333333333333, accuracy on test dataset0.9517 \n",
      "epoch 354, accurracy on training dataset0.9857, accuracy on test dataset0.9518 \n",
      "epoch 355, accurracy on training dataset0.9838, accuracy on test dataset0.9502 \n",
      "epoch 356, accurracy on training dataset0.9844166666666667, accuracy on test dataset0.9513 \n",
      "epoch 357, accurracy on training dataset0.9845166666666667, accuracy on test dataset0.9501 \n",
      "epoch 358, accurracy on training dataset0.9853833333333334, accuracy on test dataset0.9517 \n",
      "epoch 359, accurracy on training dataset0.9851, accuracy on test dataset0.9531 \n",
      "epoch 360, accurracy on training dataset0.98495, accuracy on test dataset0.9513 \n",
      "epoch 361, accurracy on training dataset0.98345, accuracy on test dataset0.9511 \n",
      "epoch 362, accurracy on training dataset0.9845, accuracy on test dataset0.9522 \n",
      "epoch 363, accurracy on training dataset0.9838, accuracy on test dataset0.9511 \n",
      "epoch 364, accurracy on training dataset0.98235, accuracy on test dataset0.952 \n",
      "epoch 365, accurracy on training dataset0.9838333333333333, accuracy on test dataset0.9504 \n",
      "epoch 366, accurracy on training dataset0.9834166666666667, accuracy on test dataset0.9494 \n",
      "epoch 367, accurracy on training dataset0.9833666666666666, accuracy on test dataset0.9507 \n",
      "epoch 368, accurracy on training dataset0.9854333333333334, accuracy on test dataset0.951 \n",
      "epoch 369, accurracy on training dataset0.9831, accuracy on test dataset0.9514 \n",
      "epoch 370, accurracy on training dataset0.984, accuracy on test dataset0.9513 \n",
      "epoch 371, accurracy on training dataset0.98615, accuracy on test dataset0.9515 \n",
      "epoch 372, accurracy on training dataset0.9853166666666666, accuracy on test dataset0.9515 \n",
      "epoch 373, accurracy on training dataset0.9852666666666666, accuracy on test dataset0.9512 \n",
      "epoch 374, accurracy on training dataset0.98515, accuracy on test dataset0.9514 \n",
      "epoch 375, accurracy on training dataset0.9869166666666667, accuracy on test dataset0.9513 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 376, accurracy on training dataset0.9851166666666666, accuracy on test dataset0.9507 \n",
      "epoch 377, accurracy on training dataset0.98455, accuracy on test dataset0.9498 \n",
      "epoch 378, accurracy on training dataset0.9832666666666666, accuracy on test dataset0.95 \n",
      "epoch 379, accurracy on training dataset0.9852166666666666, accuracy on test dataset0.9506 \n",
      "epoch 380, accurracy on training dataset0.98565, accuracy on test dataset0.9505 \n",
      "epoch 381, accurracy on training dataset0.9863666666666666, accuracy on test dataset0.9516 \n",
      "epoch 382, accurracy on training dataset0.98565, accuracy on test dataset0.9503 \n",
      "epoch 383, accurracy on training dataset0.98765, accuracy on test dataset0.9514 \n",
      "epoch 384, accurracy on training dataset0.9869166666666667, accuracy on test dataset0.9515 \n",
      "epoch 385, accurracy on training dataset0.9859166666666667, accuracy on test dataset0.9504 \n",
      "epoch 386, accurracy on training dataset0.9854, accuracy on test dataset0.9501 \n",
      "epoch 387, accurracy on training dataset0.98555, accuracy on test dataset0.9508 \n",
      "epoch 388, accurracy on training dataset0.9884666666666667, accuracy on test dataset0.9531 \n",
      "epoch 389, accurracy on training dataset0.9872333333333333, accuracy on test dataset0.9519 \n",
      "epoch 390, accurracy on training dataset0.9874333333333334, accuracy on test dataset0.9537 \n",
      "epoch 391, accurracy on training dataset0.9883, accuracy on test dataset0.9505 \n",
      "epoch 392, accurracy on training dataset0.9875333333333334, accuracy on test dataset0.952 \n",
      "epoch 393, accurracy on training dataset0.98925, accuracy on test dataset0.9516 \n",
      "epoch 394, accurracy on training dataset0.98815, accuracy on test dataset0.9511 \n",
      "epoch 395, accurracy on training dataset0.9869166666666667, accuracy on test dataset0.951 \n",
      "epoch 396, accurracy on training dataset0.9869833333333333, accuracy on test dataset0.9503 \n",
      "epoch 397, accurracy on training dataset0.9894166666666667, accuracy on test dataset0.9526 \n",
      "epoch 398, accurracy on training dataset0.9879333333333333, accuracy on test dataset0.952 \n",
      "epoch 399, accurracy on training dataset0.98785, accuracy on test dataset0.9525 \n",
      "epoch 400, accurracy on training dataset0.9895166666666667, accuracy on test dataset0.9523 \n",
      "epoch 401, accurracy on training dataset0.9867666666666667, accuracy on test dataset0.9505 \n",
      "epoch 402, accurracy on training dataset0.987, accuracy on test dataset0.9508 \n",
      "epoch 403, accurracy on training dataset0.9858666666666667, accuracy on test dataset0.9492 \n",
      "epoch 404, accurracy on training dataset0.9875666666666667, accuracy on test dataset0.9503 \n",
      "epoch 405, accurracy on training dataset0.9884166666666667, accuracy on test dataset0.9513 \n",
      "epoch 406, accurracy on training dataset0.9873833333333333, accuracy on test dataset0.95 \n",
      "epoch 407, accurracy on training dataset0.98815, accuracy on test dataset0.9522 \n",
      "epoch 408, accurracy on training dataset0.9889, accuracy on test dataset0.9523 \n",
      "epoch 409, accurracy on training dataset0.9874833333333334, accuracy on test dataset0.9503 \n",
      "epoch 410, accurracy on training dataset0.98805, accuracy on test dataset0.9507 \n",
      "epoch 411, accurracy on training dataset0.9884, accuracy on test dataset0.951 \n",
      "epoch 412, accurracy on training dataset0.9878333333333333, accuracy on test dataset0.9511 \n",
      "epoch 413, accurracy on training dataset0.9887333333333334, accuracy on test dataset0.9506 \n",
      "epoch 414, accurracy on training dataset0.9877166666666667, accuracy on test dataset0.9505 \n",
      "epoch 415, accurracy on training dataset0.98635, accuracy on test dataset0.9511 \n",
      "epoch 416, accurracy on training dataset0.9870833333333333, accuracy on test dataset0.9499 \n",
      "epoch 417, accurracy on training dataset0.9881, accuracy on test dataset0.9518 \n",
      "epoch 418, accurracy on training dataset0.9908333333333333, accuracy on test dataset0.9517 \n",
      "Finally epoch 418, accurracy on training dataset0.9908333333333333, accuracy on test dataset0.9517 \n"
     ]
    }
   ],
   "source": [
    "train_accuracy_list, test_accuracy_list,ep = fit_gd_cross_entropy2(x_train, y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
